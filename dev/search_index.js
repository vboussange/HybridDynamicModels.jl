var documenterSearchIndex = {"docs":
[{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/#Dataloaders","page":"API","title":"Dataloaders","text":"","category":"section"},{"location":"api/#HybridDynamicModels.SegmentedTimeSeries","page":"API","title":"HybridDynamicModels.SegmentedTimeSeries","text":"SegmentedTimeSeries(data; segment_length=2, shift=nothing, batchsize=1, shuffle=false, partial_segment=false, partial_batch=false, rng=GLOBAL_RNG)\n\nAn object that iterates over mini-batches of segments of data, each segment containing segment_length data points, each mini-batch containing batchsize segments. The last dimension in each tensor is the time dimension.\n\nArguments\n\ndata: Input data (array, tuple, or named tuple).\nsegment_length: Number of time points in each segment.\nshift: Step size between consecutive segments (default: segment_length).\nbatchsize: Number of segments per batch.\nshuffle: Whether to shuffle segment order.\npartial_segment: Allow shorter final segments.\npartial_batch: Allow smaller final batches.\nrng: Random number generator for shuffling.\n\nInputs\n\ndata: The time series data to segment.\n\nOutputs\n\nIterator yielding batches of data segments.\n\nBehavior\n\nCreates overlapping or non-overlapping segments from time series data for training dynamical models. Segments can be shuffled and batched for efficient training.\n\nExample\n\njulia> Xtrain = rand(10, 100)\njulia> sdl = SegmentedTimeSeries(Xtrain; segment_length=2, batchsize=1)\njulia> for batch in sdl\n           println(\"Batch: \", summary(batch))\n       end\n\n!!!warning     Undefined behavior when data dimensions are incompatible\n\n\n\n\n\n","category":"type"},{"location":"api/#HybridDynamicModels.tokenize","page":"API","title":"HybridDynamicModels.tokenize","text":"tokenize(sdl::SegmentedTimeSeries)\n\nConvert a SegmentedTimeSeries to use token-based indexing.\n\nArguments\n\nsdl: The SegmentedTimeSeries to tokenize.\n\nInputs\n\nsdl: SegmentedTimeSeries object.\n\nOutputs\n\nTokenized SegmentedTimeSeries with integer-based segment access.\n\nBehavior\n\nTransforms segment indices into a token-based system for easier access to individual segments.\n\nExample\n\njulia> sdl = SegmentedTimeSeries(rand(10, 100); segment_length=2)\njulia> tokenized_sdl = tokenize(sdl)\njulia> tokens(tokenized_sdl) # Returns available tokens\n\n\n\n\n\n","category":"function"},{"location":"api/#HybridDynamicModels.tokens","page":"API","title":"HybridDynamicModels.tokens","text":"tokens(sdl::SegmentedTimeSeries)\n\nGet the available tokens for a tokenized SegmentedTimeSeries.\n\nArguments\n\nsdl: A tokenized SegmentedTimeSeries.\n\nInputs\n\nsdl: Tokenized SegmentedTimeSeries object.\n\nOutputs\n\nRange of available tokens (1 to number of segments).\n\nBehavior\n\nReturns the range of tokens that can be used to access individual segments in a tokenized SegmentedTimeSeries.\n\nExample\n\njulia> sdl = SegmentedTimeSeries(rand(10, 100); segment_length=2)\njulia> tokenized_sdl = tokenize(sdl)\njulia> collect(tokens(tokenized_sdl)) # [1, 2, 3, ...]\n\n\n\n\n\n","category":"function"},{"location":"api/#HybridDynamicModels.create_train_val_loaders","page":"API","title":"HybridDynamicModels.create_train_val_loaders","text":"create_train_val_loaders(data; segment_length, valid_length, kwargs...)\n\nCreate separate training and validation SegmentedTimeSeries loaders from a dataset with the same number of batches.\n\nThis function splits the data into non-overlapping training and validation segments. The training data uses segments with gaps equal to valid_length to leave space for validation segments. The validation data starts after the first training segment and uses segments of length valid_length. Both loaders are guaranteed to have the same number of batches, with tokens referring to the same ordering.\n\nArguments\n\ndata: Input data (can be an array, tuple, or named tuple)\nsegment_length: Size of training segments\nvalid_length: Size of validation segments\nkwargs...: Additional arguments passed to SegmentedTimeSeries constructors\n\nReturns\n\ndataloader_train: SegmentedTimeSeries for training data\ndataloader_valid: SegmentedTimeSeries for validation data\n\nExamples\n\nWith array data\n\ndata = rand(10, 100)  # 10 features, 100 time steps\ntrain_loader, val_loader = create_train_val_loaders(data; \n    segment_length=20, valid_length=5, batchsize=4)\n# Both loaders will have the same number of batches\n@assert length(train_loader) == length(val_loader)\n\nWith tuple data (data, time steps)\n\ndata = rand(10, 100)\ntsteps = 1:100\ntrain_loader, val_loader = create_train_val_loaders((data, tsteps); \n    segment_length=20, valid_length=5, batchsize=4)\n\nWith named tuple data\n\ndataset = (observations = rand(10, 100), times = 1:100, metadata = rand(5, 100))\ntrain_loader, val_loader = create_train_val_loaders(dataset; \n    segment_length=20, valid_length=5)\n\nNotes\n\nTraining segments are spaced segment_length + valid_length apart to avoid overlap with validation\nValidation segments start at position segment_length + 1 to avoid overlap with first training segment\nBoth loaders have partial_segment = false and partial_batch = false to ensure consistent sizes\nBoth loaders are guaranteed to have the same number of batches for synchronized training/validation\n\n\n\n\n\n","category":"function"},{"location":"api/#Parameter-Layers","page":"API","title":"Parameter Layers","text":"","category":"section"},{"location":"api/#HybridDynamicModels.ParameterLayer","page":"API","title":"HybridDynamicModels.ParameterLayer","text":"ParameterLayer(;constraint::AbstractConstraint = NoConstraint(), \n                init_value = (;), \n                init_state_value = (;))\n\nA layer representing parameters, optionally with constraints.\n\nArguments\n\nconstraint: Constraint to transform parameters.\ninit_value: Initial parameter values (NamedTuple or AbstractArray).\ninit_state_value: Internal state (NamedTuple).\n\nInputs\n\nps: Parameters of the layer.\nst: States of the layer.\n\nOutputs\n\nConstrained parameter values merged with states.\n\nBehavior\n\nApplies constraints to parameters during forward pass. Parameters are transformed from unconstrained to constrained space.\n\nExample\n\njulia> param = ParameterLayer(; constraint = NoConstraint(),\n                            init_value = (;a = ones(2)), \n                            init_state_value = (;b = (0.0, 1.0)))\njulia> ps, st = Lux.setup(Random.default_rng(), param)\njulia> x, _ = param(ps, st)\njulia> x == (a = [1.0, 1.0], b = (0.0, 1.0))\ntrue\n\n\n\n\n\n","category":"type"},{"location":"api/#HybridDynamicModels.NoConstraint","page":"API","title":"HybridDynamicModels.NoConstraint","text":"NoConstraint()\n\nApplies no transformation to parameters.\n\nArguments\n\nNone.\n\nInputs\n\nx: Parameter values.\nst: States.\n\nOutputs\n\nUnmodified parameter values and states.\n\nBehavior\n\nIdentity transformation - parameters remain unconstrained.\n\nExample\n\njulia> constraint = NoConstraint()\njulia> x, st = constraint([1.0, 2.0], (;))\n([1.0, 2.0], (;))\n\n\n\n\n\n","category":"type"},{"location":"api/#HybridDynamicModels.BoxConstraint","page":"API","title":"HybridDynamicModels.BoxConstraint","text":"BoxConstraint(lb::AbstractArray, ub::AbstractArray)\n\nConstrains parameters to lie within specified bounds using sigmoid transformation.\n\nArguments\n\nlb: Lower bounds array.\nub: Upper bounds array.\n\nInputs\n\ny: Unconstrained parameter values.\nst: States containing bounds.\n\nOutputs\n\nConstrained parameter values within bounds.\n\nBehavior\n\nTransforms unconstrained parameters to constrained space using sigmoid function.\n\nExample\n\njulia> constraint = BoxConstraint([0.0], [1.0])\njulia> x, st = constraint([0.0], (;lb=[0.0], ub=[1.0]))\n([0.5], (;lb=[0.0], ub=[1.0]))\n\n\n\n\n\n","category":"type"},{"location":"api/#HybridDynamicModels.NamedTupleConstraint","page":"API","title":"HybridDynamicModels.NamedTupleConstraint","text":"NamedTupleConstraint(constraints::NamedTuple)\n\nApplies different constraints to different fields of a NamedTuple.\n\nArguments\n\nconstraints: NamedTuple of constraint objects.\n\nInputs\n\nx: NamedTuple of parameter values.\nst: NamedTuple of states.\n\nOutputs\n\nConstrained parameter values.\n\nBehavior\n\nApplies field-specific constraints to NamedTuple parameters.\n\nExample\n\njulia> constraints = (a = BoxConstraint([0.0], [1.0]), b = NoConstraint())\njulia> constraint = NamedTupleConstraint(constraints)\n\n\n\n\n\n","category":"type"},{"location":"api/#Models","page":"API","title":"Models","text":"","category":"section"},{"location":"api/#HybridDynamicModels.ODEModel","page":"API","title":"HybridDynamicModels.ODEModel","text":"ODEModel(layers::NamedTuple, dudt::Function; kwargs...)\n\nWraps an ODE model for simulation using Lux layers.\n\nArguments\n\nlayers: NamedTuple of Lux layers representing the layers of the model.\ndudt: Function that computes the derivative of the state, with signature dudt(layers, u, ps, t).\nkwargs: Additional keyword arguments passed to the solver (e.g., tspan, saveat, alg).\n\nInputs\n\n(x, ps, st)\nx: a NamedTuple or AbstractVector{NamedTuple} (batch mode).\nps: Parameters of the model.\nst: States of the model.\nA tuple of (x, ps, st): batch mode.\n(ps, st): If x not provided, defaults to kwargs.\n\nOutputs\n\n(sol, st)\nsol: Solution of the ODE problem, with second dimension corresponding to time and batches stacked along the third dimension, if applicable.\nst: Updated states of the model.\n\nBehavior\n\nlayers are wrapped in StatefulLuxLayers to maintain their states. The derivative function dudt should be defined as dudt(layers, u, ps, t) where u is the current state and t is the current time. The function returns the derivative of the state.\n\nExample\n\njulia> layers = (; layer1 = Lux.Dense(10, 10, relu))\njulia> dudt(layers, u, ps, t) = layers.layer1(u, ps.layer1)[1]\njulia> ode_model = ODEModel(layers, dudt, tspan = (0f0, 1f0), saveat = range(0f0, stop=1f0, length=100), alg = Tsit5())\njulia> ps, st = Lux.setup(Random.default_rng(), ode_model)\njulia> ode_model((; u0 = ones(Float32, 10)), ps, st)\n\n!!!warning     Undefined behavior when ps is not a NamedTuple\n\n\n\n\n\n","category":"type"},{"location":"api/#HybridDynamicModels.AnalyticModel","page":"API","title":"HybridDynamicModels.AnalyticModel","text":"AnalyticModel(layers::NamedTuple, fun::Function; kwargs...)\n\nWraps an analytic model for direct evaluation using Lux layers.\n\nArguments\n\nlayers: NamedTuple of Lux layers representing the layers of the model.\nfun: Function that computes the analytic solution, with signature fun(layers, u0, t0, ps, t).\nkwargs: Additional keyword arguments (e.g., default values for u0, tspan, saveat).\n\nInputs\n\n(x, ps, st)\nx: a NamedTuple or AbstractVector{NamedTuple} (batch mode).\nps: Parameters of the model.\nst: States of the model.\nA tuple of (x, ps, st): batch mode.\n(ps, st): If x not provided, defaults to kwargs.\n\nOutputs\n\n(sol, st)\nsol: Solution array evaluated at specified time points, with second dimension corresponding to time and batches stacked along the third dimension, if applicable.\nst: Updated states of the model.\n\nBehavior\n\nlayers are wrapped in StatefulLuxLayer to maintain their states. The analytic function fun should be defined as fun(layers, u0, t0, ps, t) where t can be a vector of time points and t0 is extracted from tspan.\n\nExample\n\njulia> layers = (; params = ParameterLayer(init_value = (a = 1.0, b = 0.5)))\njulia> analytic_solution(layers, u0, t0, ps, t) = u0 .* exp.(layers.params(ps.params)[1].a .* (t .- t0))\njulia> model = AnalyticModel(layers, analytic_solution; u0 = [1.0], tspan = (0.0, 1.0), saveat = 0:0.1:1.0)\njulia> ps, st = Lux.setup(Random.default_rng(), model)\njulia> model((; u0 = [1.0]), ps, st)\n\n!!!warning     Undefined behavior when ps is not a NamedTuple\n\n\n\n\n\n","category":"type"},{"location":"api/#HybridDynamicModels.ARModel","page":"API","title":"HybridDynamicModels.ARModel","text":"ARModel(layers::NamedTuple, fun::Function; kwargs...)\n\nWraps an autoregressive (AR) model.\n\nArguments\n\nlayers: NamedTuple of Lux layers representing the layers associated with the model.\nfun: Function that computes the next time step, with signature fun(layers, u, ps, t).\nkwargs: Additional keyword arguments (e.g., default values for u0, tspan, saveat, dt).\n\nInputs\n\n(x, ps, st)\nx: a NamedTuple or AbstractVector{NamedTuple} (batch mode).\nps: Parameters of the model.\nst: States of the model.\nA tuple of (x, ps, st): batch mode.\n(ps, st): If x not provided, defaults to kwargs.\n\nOutputs\n\n(sol, st)\nsol: Solution array with iterative predictions, with second dimension corresponding to time and batches stacked along the third dimension, if applicable.\nst: Updated states of the model.\n\nBehavior\n\nlayers are wrapped in StatefulLuxLayer to maintain their states. The AR function fun should be defined as fun(layers, u, ps, t) where:\n\nu is the current state\nt is the current time\nThe function returns the next state\n\nThe model iteratively applies the function to generate a time series from initial conditions.\n\nExample\n\njulia> using HybridDynamicModels, Lux, Random\n\njulia> layers = (; \n           predictor = Dense(2, 2), \n           params = ParameterLayer(init_value = (; decay = [95f-2],))\n       );\n\njulia> ar_step(layers, u, ps, t) = layers.predictor(u, ps.predictor) .* layers.params(ps.params).decay;\n\njulia> model = ARModel(layers, ar_step; dt = 1f-1, u0 = [1f0, 5f-1], tspan = (0f0, 1f0), saveat = 0:1f-1:1f0);\n\njulia> ps, st = Lux.setup(Random.default_rng(), model);\n\njulia> x = (; u0 = [1f0, 5f-1]);\n\njulia> y, st = model(x, ps, st);\n\njulia> size(y) # 2 state variables, 11 time points\n(2, 11) \n\n!!!warning     Undefined behavior when x is not a NamedTuple\n\n\n\n\n\n","category":"type"},{"location":"api/#Initial-Conditions","page":"API","title":"Initial Conditions","text":"","category":"section"},{"location":"api/#HybridDynamicModels.ICLayer","page":"API","title":"HybridDynamicModels.ICLayer","text":"ICLayer(ics::AbstractLuxLayer)\nICLayer(ics::<:ParameterLayer)\nICLayer(ics::Vector{<:ParameterLayer})\n\nInitial condition layer.\n\nArguments\n\nics: Lux layer, ParameterLayer, or vector of ParameterLayer.\n\nInputs\n\n(x, ps, st) with x a NamedTuple or AbstractVector{NamedTuple} (batch mode).\n(ps, st) when ics is a ParameterLayer.\n\nOutputs\n\nInitial conditions merged with other fields.\nUpdated states.\n\nBehavior\n\nProcesses initial conditions through wrapped layers and merges with input data.\n\nExample\n\njulia> ic_layer = ICLayer(ParameterLayer(init_value = (;u0 = [1.0])))\n\n\n\n\n\n","category":"type"},{"location":"api/#Training-API","page":"API","title":"Training API","text":"","category":"section"},{"location":"api/#HybridDynamicModels.train","page":"API","title":"HybridDynamicModels.train","text":"train(backend::AbstractOptimBackend, model, dataloader::SegmentedTimeSeries, infer_ics::InferICs, rng=Random.default_rng(); pstype=Lux.f64)\n\nTrain a dynamical model using segmented time series data.\n\nArguments\n\nbackend: Training configuration and optimization settings.\nmodel: Lux model to train.\ndataloader: Time series data split into segments.\ninfer_ics: Initial condition inference configuration.\nrng: Random number generator.\npstype: Precision type for parameters.\n\nInputs\n\nbackend: Training backend (SGDBackend, MCSamplingBackend, etc.).\nmodel: Model to train.\ndataloader: SegmentedTimeSeries data.\ninfer_ics: InferICs configuration.\n\nOutputs\n\nNamedTuple with training results (varies by backend).\n\nBehavior\n\nTrains model using specified backend on segmented time series data.\n\nExample\n\njulia> backend = SGDBackend(Adam(1e-3), 1000, AutoZygote(), MSELoss())\njulia> dataloader = SegmentedTimeSeries(data, segment_length=20)\njulia> infer_ics = InferICs(true)\njulia> result = train(backend, model, dataloader, infer_ics)\n\n\n\n\n\n","category":"function"},{"location":"api/#HybridDynamicModels.InferICs","page":"API","title":"HybridDynamicModels.InferICs","text":"InferICs(infer::Bool, u0_constraint=NoConstraint())\n\nConfiguration for initial condition inference in training.\n\nArguments\n\ninfer: Whether to treat initial conditions as learnable parameters.\nu0_constraint: Constraint for initial condition optimization.\n\nInputs\n\ninfer: Boolean flag for inference.\nu0_constraint: Constraint object.\n\nOutputs\n\nInferICs configuration object.\n\nBehavior\n\nControls whether initial conditions are learned or fixed during training.\n\nExample\n\njulia> infer_ics = InferICs(true, NoConstraint())\n\n\n\n\n\n","category":"type"},{"location":"api/#Lux.jl-training-backend","page":"API","title":"Lux.jl training backend","text":"","category":"section"},{"location":"api/#HybridDynamicModels.SGDBackend","page":"API","title":"HybridDynamicModels.SGDBackend","text":"SGDBackend(opt, n_epochs, adtype, loss_fn, callback)\n\nTraining backend using Lux.jl for mode estimation.\n\n!!! warning Conditional loading   You need to load Optimisers, ComponentArrays and Lux before loading HybridDynamicModels to use SGDBackend.\n\nArguments\n\nopt: Optimizers.jl rule for parameter updates.\nn_epochs: Number of training epochs.\nadtype: Automatic differentiation backend.\nloss_fn: Loss function for training.\ncallback: User-defined callback function.\n\nInputs\n\nopt: Optimization rule (e.g., Adam(1e-3)).\nn_epochs: Total training epochs.\nadtype: AD backend (e.g., AutoZygote()).\nloss_fn: Loss function.\ncallback: Optional callback.\n\nOutputs\n\nNamedTuple with trained parameters and states.\n\nBehavior\n\nUses stochastic gradient descent for maximum likelihood estimation.\n\nExample\n\njulia> backend = SGDBackend(Adam(1e-3), 1000, AutoZygote(), MSELoss())\n\n\n\n\n\n","category":"type"},{"location":"api/#Turing.jl-backend","page":"API","title":"Turing.jl backend","text":"","category":"section"},{"location":"api/#HybridDynamicModels.BayesianLayer","page":"API","title":"HybridDynamicModels.BayesianLayer","text":"BayesianLayer(layer, priors)\n\nWrapper layer that adds Bayesian priors to Lux layers for probabilistic modeling.\n\nArguments\n\nlayer: Lux layer to make Bayesian.\npriors: Prior distributions for parameters.\n\nInputs\n\nlayer: Any Lux layer.\npriors: Distribution or NamedTuple of distributions.\n\nOutputs\n\nLayer with Bayesian priors for MCMC inference.\n\nBehavior\n\nEnables probabilistic modeling by attaching priors to layer parameters.\n\nExample\n\njulia> dense_layer = Dense(10, 5)\njulia> bayesian_dense = BayesianLayer(dense_layer, Normal(0, 1))\n\n\n\n\n\n","category":"type"},{"location":"api/#HybridDynamicModels.getpriors","page":"API","title":"HybridDynamicModels.getpriors","text":"getpriors(layer)\n\nExtract prior distributions from Bayesian layers.\n\nArguments\n\nlayer: Layer or model containing BayesianLayer components.\n\nInputs\n\nlayer: Bayesian or composite layer.\n\nOutputs\n\nNamedTuple of prior distributions.\n\nBehavior\n\nRecursively extracts priors from Bayesian layers in model hierarchy.\n\nExample\n\njulia> priors = getpriors(bayesian_model)\n\n\n\n\n\n","category":"function"},{"location":"api/#HybridDynamicModels.create_turing_model","page":"API","title":"HybridDynamicModels.create_turing_model","text":"create_turing_model(ps_priors, data_distrib, st_model)\n\nCreate a Turing model for Bayesian inference from a BayesianLayer model.\n\nArguments\n\nps_priors: A nested structure (typically a NamedTuple) containing prior distributions for model parameters. Each leaf should be a Distributions.Distribution.\ndata_distrib: A function or distribution constructor that creates the likelihood distribution for observed data points.\nst_model: A stateful Lux model that can be called with parameters to generate predictions.\n\nReturns\n\nA function (xs, ys) -> Turing.Model that creates a Turing model when given input data xs and observed data ys.\n\n\n\n\n\n","category":"function"},{"location":"api/#HybridDynamicModels.MCSamplingBackend","page":"API","title":"HybridDynamicModels.MCSamplingBackend","text":"MCSamplingBackend(sampler, n_iterations, datadistrib; kwargs...)\n\nTraining backend for Bayesian inference using Monte Carlo sampling. !!! warning Conditional loading   You need to load Turing, ComponentArrays and Lux before loading HybridDynamicModels to use MCSamplingBackend.\n\nArguments\n\nsampler: Turing.jl MCMC sampler.\nn_iterations: Number of MCMC samples.\ndatadistrib: Data distribution for likelihood.\nkwargs: Additional sampler options.\n\nInputs\n\nsampler: MCMC sampling algorithm.\nn_iterations: Total posterior samples.\ndatadistrib: Distribution for data likelihood.\n\nOutputs\n\nNamedTuple with MCMC chains and model state.\n\nBehavior\n\nPerforms Bayesian inference using MCMC sampling on models with priors.\n\nExample\n\njulia> backend = MCSamplingBackend(NUTS(0.65), 1000, LogNormal)\n\n\n\n\n\n","category":"type"},{"location":"examples/customtraining_example/#Overloading-the-train-function","page":"Overloading the train function","title":"Overloading the train function","text":"","category":"section"},{"location":"examples/customtraining_example/","page":"Overloading the train function","title":"Overloading the train function","text":"The default train function is opiniated and meant for demonstration purposes. For more advanced training, you should create your own training pipeline.","category":"page"},{"location":"examples/customtraining_example/","page":"Overloading the train function","title":"Overloading the train function","text":"This tutorial shows how to build a custom training pipeline that includes validation to prevent overfitting.","category":"page"},{"location":"examples/customtraining_example/#Define-a-custom-setup-struct","page":"Overloading the train function","title":"Define a custom setup struct","text":"","category":"section"},{"location":"examples/customtraining_example/","page":"Overloading the train function","title":"Overloading the train function","text":"Create a new struct that inherits from AbstractSetup. This struct will hold the validation dataloader.","category":"page"},{"location":"examples/customtraining_example/","page":"Overloading the train function","title":"Overloading the train function","text":"using HybridDynamicModels\nimport HybridDynamicModels: AbstractSetup\nimport LuxCore: AbstractLuxLayer\nimport ConcreteStructs: @concrete\nimport Random\n\n@concrete struct WithValidation <: AbstractSetup\n    dataloader\nend","category":"page"},{"location":"examples/customtraining_example/#Implement-the-custom-train-method","page":"Overloading the train function","title":"Implement the custom train method","text":"","category":"section"},{"location":"examples/customtraining_example/","page":"Overloading the train function","title":"Overloading the train function","text":"Implement a train method that takes your custom setup. This method will train the model while monitoring validation loss.","category":"page"},{"location":"examples/customtraining_example/","page":"Overloading the train function","title":"Overloading the train function","text":"The method performs these steps:","category":"page"},{"location":"examples/customtraining_example/","page":"Overloading the train function","title":"Overloading the train function","text":"Prepare the training and validation data\nSet up the model with a feature wrapper\nInitialize the training state\nTrain for multiple epochs, computing both training and validation loss\nSave the best model parameters based on validation loss","category":"page"},{"location":"examples/customtraining_example/","page":"Overloading the train function","title":"Overloading the train function","text":"function feature_wrapper((batched_segments, batched_tsteps))\n    return [(; u0 = batched_segments[:, 1, i],\n                saveat = batched_tsteps[:, i],\n                tspan = (batched_tsteps[1, i], batched_tsteps[end, i])\n            )\n            for i in 1:size(batched_tsteps, 2)]\nend\n\nfunction HybridDynamicModels.train(backend::SGDBackend,\n        model::AbstractLuxLayer,\n        dataloader_train::SegmentedTimeSeries,\n        experimental_setup::WithValidation,\n        rng = Random.default_rng(),\n        luxtype = Lux.f64)\n    dataloader_train = luxtype(dataloader_train)\n    dataloader_valid = luxtype(experimental_setup.dataloader)\n\n    @assert length(dataloader_train)==length(dataloader_valid) \"The training and validation dataloaders must have the same number of segments\"\n\n    model_with_wrapper = Chain((; wrapper = Lux.WrappedFunction(feature_wrapper), model = model))\n\n    ps, st = luxtype(Lux.setup(rng, model_with_wrapper))\n\n    train_state = Training.TrainState(model_with_wrapper, ps, st, backend.opt)\n    best_ps = ps.model\n    best_st = st.model\n\n    best_loss = luxtype(Inf)\n    for epoch in 1:(backend.n_epochs)\n        train_loss = luxtype(0.0)\n        for (batched_segments, batched_tsteps) in dataloader_train\n            _, loss, _, train_state = Training.single_train_step!(\n                backend.adtype,\n                backend.loss_fn,\n                ((batched_segments, batched_tsteps), batched_segments),\n                train_state)\n            train_loss += loss\n        end\n\n        valid_loss = 0.0\n        ps, st = train_state.parameters, train_state.states\n\n        for (batched_segments, batched_data) in dataloader_valid\n            segment_pred, _ = model_with_wrapper((batched_segments, batched_data), ps, st)\n            valid_loss += backend.loss_fn(segment_pred, batched_segments)\n        end\n\n        println(\"Train loss: $train_loss\")\n        println(\"Validation loss: $valid_loss\")\n        if valid_loss < best_loss\n            best_ps = ps.model\n            best_st = st.model\n            best_loss = train_loss\n        end\n    end\n\n    return (; ps = best_ps, st = best_st)\nend","category":"page"},{"location":"examples/customtraining_example/#Training-example","page":"Overloading the train function","title":"Training example","text":"","category":"section"},{"location":"examples/customtraining_example/","page":"Overloading the train function","title":"Overloading the train function","text":"To use this custom pipeline, create training and validation dataloaders, set up the custom configuration, and call the train function.","category":"page"},{"location":"examples/customtraining_example/","page":"Overloading the train function","title":"Overloading the train function","text":"using Lux, Optimisers, ComponentArrays\nusing Zygote\n\ntsteps = range(0, stop=20.0, length=201)\ndata = randn(2, length(tsteps))\n\nsegment_length = 20\nvalid_length = 2\nbatchsize = 4\n\ndataloader_train, dataloader_valid = create_train_val_loaders((data, tsteps);\n                                                                segment_length,\n                                                                valid_length,\n                                                                batchsize,\n                                                                partial_batch = true)\n\nsetup = WithValidation(dataloader_valid)\n\nnn = Chain(\n    Dense(2, 16, relu),\n    Dense(16, 16, relu),\n    Dense(16, 2)\n)\n\nfunction ar_step(layers, u, ps, t)\n    return layers.nn(u, ps.nn)\nend\n\nmodel = ARModel(\n    (;nn),\n    ar_step;\n    dt = tsteps[2] - tsteps[1],\n)\n\nbackend = SGDBackend(Adam(0.01),\n                    10,\n                    AutoZygote(),\n                    MSELoss())\n\ntrain(backend, model, dataloader_train, setup);","category":"page"},{"location":"examples/customtraining_example/","page":"Overloading the train function","title":"Overloading the train function","text":"Train loss: 2.109421017357195\nValidation loss: 1.4409056096088366\nTrain loss: 1.9472862797880328\nValidation loss: 1.530507448297667\nTrain loss: 1.9257307652457554\nValidation loss: 1.6700692103016448\nTrain loss: 5.03948890131515\nValidation loss: 1.6481922673825224\nTrain loss: 1.904333240487691\nValidation loss: 1.767064890350757\nTrain loss: 1.917032504518063\nValidation loss: 1.8808901356109418\nTrain loss: 1.9528679899876793\nValidation loss: 1.8028009947653298\nTrain loss: 1.9166707069227633\nValidation loss: 1.76441670937343\nTrain loss: 1.924461033019612\nValidation loss: 1.751831104980442\nTrain loss: 1.9364433302967283\nValidation loss: 1.751272399727613","category":"page"},{"location":"examples/sgd_example/#Training-with-[SGDBackend](@ref)","page":"Training with SGDBackend","title":"Training with SGDBackend","text":"","category":"section"},{"location":"examples/sgd_example/","page":"Training with SGDBackend","title":"Training with SGDBackend","text":"This tutorial demonstrates how to use the SGDBackend for training a hybrid autoregressive model modelling the classic hare-lynx predator-prey system, where the predation interaction is learned via neural networks while maintaining mechanistic constraints for birth and death processes.","category":"page"},{"location":"examples/sgd_example/","page":"Training with SGDBackend","title":"Training with SGDBackend","text":"note: Note\nThe train function provided by HybridDynamicModels is an experimental feature, exposed for demonstration purposes. Users are encouraged to implement their own train function to gain more control over the training process; see Overloading the train function.","category":"page"},{"location":"examples/sgd_example/#Importing-necessary-packages","page":"Training with SGDBackend","title":"Importing necessary packages","text":"","category":"section"},{"location":"examples/sgd_example/","page":"Training with SGDBackend","title":"Training with SGDBackend","text":"In order to use the SGDBackend, we'll need to manually load Lux, Optimisers, and ComponentArrays. We additionally load Zygote for automatic differentiation, ParameterSchedulers for learning rate scheduling, and Plots, DataFrames, DelimitedFiles, and HTTP for data handling and visualization.","category":"page"},{"location":"examples/sgd_example/","page":"Training with SGDBackend","title":"Training with SGDBackend","text":"using Lux, Optimisers, ComponentArrays\nusing Zygote\nusing HybridDynamicModels\nusing ParameterSchedulers\nusing Random\nusing Plots\nusing DataFrames, DelimitedFiles, HTTP\n\nconst luxtype = Lux.f64","category":"page"},{"location":"examples/sgd_example/","page":"Training with SGDBackend","title":"Training with SGDBackend","text":"f64 (generic function with 1 method)","category":"page"},{"location":"examples/sgd_example/#Data-loading","page":"Training with SGDBackend","title":"Data loading","text":"","category":"section"},{"location":"examples/sgd_example/","page":"Training with SGDBackend","title":"Training with SGDBackend","text":"Load the Lynx-Hare population dataset:","category":"page"},{"location":"examples/sgd_example/","page":"Training with SGDBackend","title":"Training with SGDBackend","text":"url = \"http://people.whitman.edu/~hundledr/courses/M250F03/LynxHare.txt\"\ndata = readdlm(IOBuffer(HTTP.get(url).body), ' ') |> luxtype\ndf_data = DataFrame(Year = data[:, 1], Hare = data[:, 2], Lynx = data[:, 3])\n\n# Visualize observed data (hare and lynx)\nplt_data = plot(df_data.Year, df_data.Hare, label = \"Hare\", xlabel = \"Year\",\n    ylabel = \"Population\", title = \"Observed Hare-Lynx Data\")\nplot!(plt_data, df_data.Year, df_data.Lynx, label = \"Lynx\")\ndisplay(plt_data)","category":"page"},{"location":"examples/sgd_example/","page":"Training with SGDBackend","title":"Training with SGDBackend","text":"(Image: )","category":"page"},{"location":"examples/sgd_example/#Data-preparation","page":"Training with SGDBackend","title":"Data preparation","text":"","category":"section"},{"location":"examples/sgd_example/","page":"Training with SGDBackend","title":"Training with SGDBackend","text":"Prepare training and test datasets:","category":"page"},{"location":"examples/sgd_example/","page":"Training with SGDBackend","title":"Training with SGDBackend","text":"tsteps = Vector(df_data.Year) |> luxtype\n\n# Extract hare and lynx data\nhare_lynx_data = Array(df_data[:, Not(:Year)])' |> luxtype\nhare_lynx_data ./= maximum(hare_lynx_data)\n\n# Data array: [hare, lynx]\ndata_array = hare_lynx_data |> luxtype\n\nforecast_length = 20\ntest_idx = size(data_array, 2) - forecast_length + 1:size(data_array, 2)\n\n# Create training dataloader\ndataloader_train = SegmentedTimeSeries(\n    (data_array[:, Not(test_idx)], tsteps[Not(test_idx)]);\n    segment_length = 4, shift = 2, batchsize = 20)","category":"page"},{"location":"examples/sgd_example/","page":"Training with SGDBackend","title":"Training with SGDBackend","text":"SegmentedTimeSeries\n  Time series length: 71\n  Segment length: 4\n  Shift: 2 (50.0% overlap)\n  Batch size: 20\n  Total segments: 34\n  Total batches: 1","category":"page"},{"location":"examples/sgd_example/#Model-definition","page":"Training with SGDBackend","title":"Model definition","text":"","category":"section"},{"location":"examples/sgd_example/","page":"Training with SGDBackend","title":"Training with SGDBackend","text":"Define a hare-lynx predator-prey model where the predation interaction is learned via neural networks, while birth and death processes follow mechanistic rules:","category":"page"},{"location":"examples/sgd_example/","page":"Training with SGDBackend","title":"Training with SGDBackend","text":"# Neural network for hare-lynx predation interactions\nhlsize = 2^4\nneural_interactions = Chain(Dense(2, hlsize, relu),\n                        Dense(hlsize, hlsize, relu),\n                        Dense(hlsize, 1))  # Output: predation rate\n\n# Learnable ecological parameters\nmechanistic_params = ParameterLayer(init_value = (\n                                    hare_birth = [0.8],\n                                    hare_death = [0.1],\n                                    lynx_death = [0.2] ), \n                                    constraint = NamedTupleConstraint((hare_birth = BoxConstraint([0.0], [2.0]),\n                                                                       hare_death = BoxConstraint([0.001], [1.0]),\n                                                                       lynx_death = BoxConstraint([0.001], [1.0]))\n                                ))\n\n# Hybrid ecosystem dynamics\nfunction ecosystem_step(layers, u, ps, t)\n    hare, lynx = max.(u, 0.)  # Unpack state variables\n    \n    params = layers.mechanistic_params(ps.mechanistic_params)\n    \n    # Neural network: predation rate\n    predation_input = [hare, lynx]\n    predation_rate = layers.neural_interactions(predation_input, ps.neural_interactions)[1]\n    \n    # Mechanistic hare dynamics\n    hare_birth = params.hare_birth[1] * hare\n    hare_predation = -predation_rate * hare * lynx\n    hare_natural_death = -params.hare_death[1] * hare\n    \n    # Mechanistic lynx dynamics\n    lynx_predation_gain = predation_rate * hare * lynx  # Lynx gain from predation\n    lynx_death = -params.lynx_death[1] * lynx\n    \n    # Return derivatives\n    return [\n        hare_birth + hare_predation + hare_natural_death,  # Hare\n        lynx_predation_gain + lynx_death                   # Lynx\n    ]\nend\n\n# Create autoregressive model\nmodel = ARModel(\n    (;neural_interactions, mechanistic_params),\n    ecosystem_step;\n    dt = tsteps[2] - tsteps[1],\n);","category":"page"},{"location":"examples/sgd_example/#Training-configuration","page":"Training with SGDBackend","title":"Training configuration","text":"","category":"section"},{"location":"examples/sgd_example/","page":"Training with SGDBackend","title":"Training with SGDBackend","text":"Configure training with learning rate scheduling and callbacks:","category":"page"},{"location":"examples/sgd_example/","page":"Training with SGDBackend","title":"Training with SGDBackend","text":"# Learning rate schedule: exponential decay\nlr_schedule = Step(1e-2, 0.9, 200)\n\n# Callback for monitoring and learning rate adjustment\nfunction callback(loss, epoch, ts)\n    if epoch % 20 == 0\n        current_lr = lr_schedule(epoch)\n        @info \"Epoch $epoch: Loss = $loss, LR = $current_lr\"\n        Optimisers.adjust!(ts.optimizer_state, current_lr)\n    end\nend\n\n# Training backend configuration\nbackend = SGDBackend(\n    AdamW(eta = 1e-2, lambda = 1e-4),  # Optimizer with weight decay\n    2000,                             # Number of epochs\n    AutoZygote(),                     # Automatic differentiation\n    MSELoss(),                        # Loss function\n    callback                          # Training callback\n)","category":"page"},{"location":"examples/sgd_example/","page":"Training with SGDBackend","title":"Training with SGDBackend","text":"HybridDynamicModelsLuxExt.SGDBackend(AdamW(eta=0.01, beta=(0.9, 0.999), lam\nbda=0.0001, epsilon=1.0e-8, couple=true), 2000, AutoZygote(), GenericLossFu\nnction{typeof(Lux.LossFunctionImpl.l2_distance_loss), typeof(mean)}(Lux.Los\nsFunctionImpl.l2_distance_loss, Statistics.mean), Main.var\"##WeaveSandBox#2\n37\".callback)","category":"page"},{"location":"examples/sgd_example/#Training","page":"Training with SGDBackend","title":"Training","text":"","category":"section"},{"location":"examples/sgd_example/","page":"Training with SGDBackend","title":"Training with SGDBackend","text":"Train the model with initial condition inference:","category":"page"},{"location":"examples/sgd_example/","page":"Training with SGDBackend","title":"Training with SGDBackend","text":"@info \"Starting training...\"\nresult = train(backend, model, dataloader_train, InferICs(true));","category":"page"},{"location":"examples/sgd_example/","page":"Training with SGDBackend","title":"Training with SGDBackend","text":"Starting training...\nEpoch 20: Loss = 0.03852390256665861, LR = 0.01\nEpoch 40: Loss = 0.027942653787698585, LR = 0.01\nEpoch 60: Loss = 0.02250271607499877, LR = 0.01\nEpoch 80: Loss = 0.019081442548950715, LR = 0.01\nEpoch 100: Loss = 0.01677088371665254, LR = 0.01\nEpoch 120: Loss = 0.014948428406206873, LR = 0.01\nEpoch 140: Loss = 0.013641223083868887, LR = 0.01\nEpoch 160: Loss = 0.012760369858494275, LR = 0.01\nEpoch 180: Loss = 0.012191515221931466, LR = 0.01\nEpoch 200: Loss = 0.011783984025547149, LR = 0.01\nEpoch 220: Loss = 0.011582293675822227, LR = 0.009000000000000001\nEpoch 240: Loss = 0.011327571686396148, LR = 0.009000000000000001\nEpoch 260: Loss = 0.011113599573799099, LR = 0.009000000000000001\nEpoch 280: Loss = 0.010929505600762465, LR = 0.009000000000000001\nEpoch 300: Loss = 0.01078854188818167, LR = 0.009000000000000001\nEpoch 320: Loss = 0.010855301665941602, LR = 0.009000000000000001\nEpoch 340: Loss = 0.01069925830545311, LR = 0.009000000000000001\nEpoch 360: Loss = 0.010574263832301346, LR = 0.009000000000000001\nEpoch 380: Loss = 0.010515642816789274, LR = 0.009000000000000001\nEpoch 400: Loss = 0.010476654497847144, LR = 0.009000000000000001\nEpoch 420: Loss = 0.010402406008279717, LR = 0.008100000000000001\nEpoch 440: Loss = 0.010413336710450062, LR = 0.008100000000000001\nEpoch 460: Loss = 0.0103137115800394, LR = 0.008100000000000001\nEpoch 480: Loss = 0.010460442344612944, LR = 0.008100000000000001\nEpoch 500: Loss = 0.010358551703934581, LR = 0.008100000000000001\nEpoch 520: Loss = 0.010234181528831937, LR = 0.008100000000000001\nEpoch 540: Loss = 0.010198570301109567, LR = 0.008100000000000001\nEpoch 560: Loss = 0.010201057124489593, LR = 0.008100000000000001\nEpoch 580: Loss = 0.010175490960825654, LR = 0.008100000000000001\nEpoch 600: Loss = 0.010148767947423398, LR = 0.008100000000000001\nEpoch 620: Loss = 0.010126690534691096, LR = 0.007290000000000001\nEpoch 640: Loss = 0.010126157089523118, LR = 0.007290000000000001\nEpoch 660: Loss = 0.010097970130386191, LR = 0.007290000000000001\nEpoch 680: Loss = 0.010092753927523124, LR = 0.007290000000000001\nEpoch 700: Loss = 0.010048444292049008, LR = 0.007290000000000001\nEpoch 720: Loss = 0.01007267616535921, LR = 0.007290000000000001\nEpoch 740: Loss = 0.010031066459617759, LR = 0.007290000000000001\nEpoch 760: Loss = 0.009972669389240673, LR = 0.007290000000000001\nEpoch 780: Loss = 0.009977638170989553, LR = 0.007290000000000001\nEpoch 800: Loss = 0.009924938186867165, LR = 0.007290000000000001\nEpoch 820: Loss = 0.00989530553470624, LR = 0.006561\nEpoch 840: Loss = 0.009888983998685829, LR = 0.006561\nEpoch 860: Loss = 0.00984115097200619, LR = 0.006561\nEpoch 880: Loss = 0.009842218112453519, LR = 0.006561\nEpoch 900: Loss = 0.00983928182925604, LR = 0.006561\nEpoch 920: Loss = 0.009857743132305636, LR = 0.006561\nEpoch 940: Loss = 0.009876043038168686, LR = 0.006561\nEpoch 960: Loss = 0.009807430974256558, LR = 0.006561\nEpoch 980: Loss = 0.009813609786593448, LR = 0.006561\nEpoch 1000: Loss = 0.009775720684797467, LR = 0.006561\nEpoch 1020: Loss = 0.009756902627900915, LR = 0.005904900000000001\nEpoch 1040: Loss = 0.009734910122518089, LR = 0.005904900000000001\nEpoch 1060: Loss = 0.009736572453970868, LR = 0.005904900000000001\nEpoch 1080: Loss = 0.009695653028908454, LR = 0.005904900000000001\nEpoch 1100: Loss = 0.009690180776037876, LR = 0.005904900000000001\nEpoch 1120: Loss = 0.009681066665797943, LR = 0.005904900000000001\nEpoch 1140: Loss = 0.009691447563739044, LR = 0.005904900000000001\nEpoch 1160: Loss = 0.009653933606235676, LR = 0.005904900000000001\nEpoch 1180: Loss = 0.009651662537390979, LR = 0.005904900000000001\nEpoch 1200: Loss = 0.00961267119994719, LR = 0.005904900000000001\nEpoch 1220: Loss = 0.00959466092790905, LR = 0.00531441\nEpoch 1240: Loss = 0.009640911316278958, LR = 0.00531441\nEpoch 1260: Loss = 0.009625967457883518, LR = 0.00531441\nEpoch 1280: Loss = 0.009589159011959758, LR = 0.00531441\nEpoch 1300: Loss = 0.00957232060316518, LR = 0.00531441\nEpoch 1320: Loss = 0.009556778434228228, LR = 0.00531441\nEpoch 1340: Loss = 0.009553528432329492, LR = 0.00531441\nEpoch 1360: Loss = 0.00957312701430117, LR = 0.00531441\nEpoch 1380: Loss = 0.009549195222161794, LR = 0.00531441\nEpoch 1400: Loss = 0.009518524896312547, LR = 0.00531441\nEpoch 1420: Loss = 0.009509621204011973, LR = 0.004782969000000001\nEpoch 1440: Loss = 0.009507915548140865, LR = 0.004782969000000001\nEpoch 1460: Loss = 0.009492913743397502, LR = 0.004782969000000001\nEpoch 1480: Loss = 0.00950210486667424, LR = 0.004782969000000001\nEpoch 1500: Loss = 0.009469310019276355, LR = 0.004782969000000001\nEpoch 1520: Loss = 0.009473242170192967, LR = 0.004782969000000001\nEpoch 1540: Loss = 0.009461959459910551, LR = 0.004782969000000001\nEpoch 1560: Loss = 0.009433661542865224, LR = 0.004782969000000001\nEpoch 1580: Loss = 0.009429006789562636, LR = 0.004782969000000001\nEpoch 1600: Loss = 0.009439658722451802, LR = 0.004782969000000001\nEpoch 1620: Loss = 0.009440601870078866, LR = 0.004304672100000001\nEpoch 1640: Loss = 0.009416706465179402, LR = 0.004304672100000001\nEpoch 1660: Loss = 0.009447811486114016, LR = 0.004304672100000001\nEpoch 1680: Loss = 0.009402547726690717, LR = 0.004304672100000001\nEpoch 1700: Loss = 0.009449940304928964, LR = 0.004304672100000001\nEpoch 1720: Loss = 0.009430129606907417, LR = 0.004304672100000001\nEpoch 1740: Loss = 0.009508126557532495, LR = 0.004304672100000001\nEpoch 1760: Loss = 0.009400520250567479, LR = 0.004304672100000001\nEpoch 1780: Loss = 0.009632991513365382, LR = 0.004304672100000001\nEpoch 1800: Loss = 0.009417936183042444, LR = 0.004304672100000001\nEpoch 1820: Loss = 0.009366173417934545, LR = 0.003874204890000001\nEpoch 1840: Loss = 0.009350439449629927, LR = 0.003874204890000001\nEpoch 1860: Loss = 0.009337450074891614, LR = 0.003874204890000001\nEpoch 1880: Loss = 0.009334827919073559, LR = 0.003874204890000001\nEpoch 1900: Loss = 0.009336032492927481, LR = 0.003874204890000001\nEpoch 1920: Loss = 0.00933159160717788, LR = 0.003874204890000001\nEpoch 1940: Loss = 0.00932713428665372, LR = 0.003874204890000001\nEpoch 1960: Loss = 0.009342276124313095, LR = 0.003874204890000001\nEpoch 1980: Loss = 0.009316025686957469, LR = 0.003874204890000001\nEpoch 2000: Loss = 0.009305595914864328, LR = 0.003874204890000001","category":"page"},{"location":"examples/sgd_example/#Results-visualization","page":"Training with SGDBackend","title":"Results visualization","text":"","category":"section"},{"location":"examples/sgd_example/","page":"Training with SGDBackend","title":"Training with SGDBackend","text":"Visualize training fit and test predictions for the hare-lynx ecosystem:","category":"page"},{"location":"examples/sgd_example/","page":"Training with SGDBackend","title":"Training with SGDBackend","text":"\n# Colors: blue for hare, red for lynx\nhare_color = \"#ffd166\"\nlynx_color = \"#ef476f\"\n\n# Function to plot training results\nfunction plot_training_results(dataloader, result, model)\n    plt = plot(title = \"Training Results\", xlabel = \"Year\",\n        ylabel = \"Population\", legend = :topright)\n\n    dataloader_tokenized = tokenize(dataloader)\n\n    for tok in tokens(dataloader_tokenized)\n        segment_data, segment_tsteps = dataloader_tokenized[tok]\n        ics = result.ics[tok].u0\n\n        pred, _ = model(\n            (; u0 = ics, saveat = segment_tsteps,\n                tspan = (segment_tsteps[1], segment_tsteps[end])),\n            result.ps, result.st)\n\n        # Plot observed data\n        scatter!(plt, segment_tsteps, segment_data[1, :],\n            label = (tok == 1 ? \"Hare Data\" : \"\"),\n            color = hare_color, markersize = 4, alpha = 0.7)\n        scatter!(plt, segment_tsteps, segment_data[2, :],\n            label = (tok == 1 ? \"Lynx Data\" : \"\"),\n            color = lynx_color, markersize = 4, alpha = 0.7)\n\n        # Plot predictions\n        plot!(plt, segment_tsteps, pred[1, :],\n            label = (tok == 1 ? \"Hare Predicted\" : \"\"),\n            color = hare_color, linewidth = 2)\n        plot!(plt, segment_tsteps, pred[2, :],\n            label = (tok == 1 ? \"Lynx Predicted\" : \"\"),\n            color = lynx_color, linewidth = 2)\n    end\n    return plt\nend\n\n# Plot training results\nplt_train = plot_training_results(dataloader_train, result, model)","category":"page"},{"location":"examples/sgd_example/","page":"Training with SGDBackend","title":"Training with SGDBackend","text":"(Image: )","category":"page"},{"location":"examples/sgd_example/","page":"Training with SGDBackend","title":"Training with SGDBackend","text":"Forecast on test data:","category":"page"},{"location":"examples/sgd_example/","page":"Training with SGDBackend","title":"Training with SGDBackend","text":"tsteps_test = tsteps[test_idx]\ndata_test = data_array[:, test_idx]\nu0, t0 = result.ics[end]\n\npreds, _ = model((; u0 = u0, tspan = (t0, tsteps_test[end]), saveat = tsteps_test),\n                result.ps, result.st)\n\n# Plot test predictions\nplt_test = plot(title = \"Test Predictions\", xlabel = \"Year\", ylabel = \"Population\", legend = :topright)\nscatter!(plt_test, tsteps_test, data_test[1, :], label = \"Hare Data\", color = hare_color, markersize = 4, alpha = 0.7)\nscatter!(plt_test, tsteps_test, data_test[2, :], label = \"Lynx Data\", color = lynx_color, markersize = 4, alpha = 0.7)\nplot!(plt_test, tsteps_test, preds[1, :], label = \"Hare Predicted\", color = hare_color, linewidth = 2)\nplot!(plt_test, tsteps_test, preds[2, :], label = \"Lynx Predicted\", color = lynx_color, linewidth = 2)","category":"page"},{"location":"examples/sgd_example/","page":"Training with SGDBackend","title":"Training with SGDBackend","text":"(Image: )","category":"page"},{"location":"examples/sgd_example/#Some-final-notes","page":"Training with SGDBackend","title":"Some final notes","text":"","category":"section"},{"location":"examples/sgd_example/","page":"Training with SGDBackend","title":"Training with SGDBackend","text":"When training a neural network-based parametrization, it is usually best practice to use a validation loss to avoid overfitting. This can be implemented by creating a separate validation dataloader (see create_train_val_loaders) and modifying the training loop to compute validation loss at intervals, overloading the train function. Check out the Overloading the train function tutorial for an example.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Documentation Build Status Julia Testing\n(Image: docs-stable) (Image: docs-dev) (Image: CI) (Image: Julia) (Image: Code Style: Blue) (Image: Aqua QA) (Image: codecov)","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: )","category":"page"},{"location":"#HybridDynamicModels.jl","page":"Home","title":"HybridDynamicModels.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Lux.jl layers and utilities to build and train hybrid dynamic models.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"HybridDynamicModels.jl is a toolbox for easily building and training hybrid dynamic models which combine mechanistic and data driven components. Built on top of the deep learning framework Lux.jl, it enables both gradient descent optimization and Bayesian inference.","category":"page"},{"location":"#Key-Features","page":"Home","title":"🚀 Key Features","text":"","category":"section"},{"location":"#**Dynamic-model-layers**","page":"Home","title":"Dynamic model layers","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ICLayer: For initial condition inference\nODEModel: Neural ODEs\nARModel: Autoregressive models\nAnalyticModel: For explicit dynamical models","category":"page"},{"location":"#**Utility-layers-for-hybrid-modeling**","page":"Home","title":"Utility layers for hybrid modeling","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ParameterLayer: Learnable parameters, composable with optional Constraint layers\nBayesianLayer: Add probabilistic priors to any Lux layer","category":"page"},{"location":"#**Data-loaders**","page":"Home","title":"Data loaders","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"SegmentedTimeSeries: Time series data loader with segmentation, implementing mini-batching.","category":"page"},{"location":"#**Training-API,-with-following-backends**","page":"Home","title":"Training API, with following backends","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"SGDBackend: Gradient descent optimization with Optimisers.jl and Lux.jl training API\nMCSamplingBackend: Full Bayesian inference with uncertainty quantification using DynamicPPL.jl and Turing.jl.","category":"page"},{"location":"#Installation","page":"Home","title":"📦 Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"HybridDynamicModels\")","category":"page"},{"location":"#Quick-Start","page":"Home","title":"🔥 Quick Start","text":"","category":"section"},{"location":"#Autoregressive-hybrid-model","page":"Home","title":"Autoregressive hybrid model","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using Lux\nusing HybridDynamicModels\nusing Random\n\n# Dense layer for interactions\ninteraction_layer = Dense(2, 2, tanh)\n\n# Parameter layer for growth/decay rates\nrate_params = ParameterLayer(init_value = (growth = [0.1], decay = [0.05]),\n    constraint = NamedTupleConstraint((; growth = BoxConstraint([0.0], [1.0]),\n        decay = BoxConstraint([0.0], [1.0]))\n    )\n)\n\n# Simple hybrid dynamics: linear terms + neural interactions\nfunction ar_step(layers, u, ps, t)\n    # Linear terms from parameters\n    params = layers.rates(ps.rates)\n    growth = vcat(params.growth, -params.decay)\n\n    # Neural network interactions\n    interactions = layers.interaction(u, ps.interaction)\n\n    return u .* (growth + interactions)\nend\n\n# Create autoregressive model\nmodel = ARModel(\n    (interaction = interaction_layer, rates = rate_params),\n    ar_step;\n    dt = 0.1)\n\n# Setup and train\nps, st = Lux.setup(Random.default_rng(), model)\ntsteps = range(0, stop = 10.0, step = 0.1)\n\npreds, _ = model(\n    (; u0 = [1.0, 1.0],\n        tspan = (tsteps[1], tsteps[end]),\n        saveat = tsteps), ps, st)\nsize(preds)  # (2, 101)","category":"page"},{"location":"","page":"Home","title":"Home","text":"We can predict batches of time series by providing a batch of initial conditions.","category":"page"},{"location":"","page":"Home","title":"Home","text":"x = [(; u0 = rand(2),\n         tspan = (tsteps[1], tsteps[end]),\n         saveat = tsteps) for _ in 1:5]\nbatch_preds, _ = model(x, ps, st)\nsize(batch_preds)  # (2, 101, 5)","category":"page"},{"location":"","page":"Home","title":"Home","text":"We can bind our model with an additional layer predicting initial conditions from some other input data using the ICLayer.","category":"page"},{"location":"","page":"Home","title":"Home","text":"ic_layer = ICLayer(Dense(10, 2, tanh))\nmodel_with_ic = Chain(ic_layer, model)\nps, st = Lux.setup(Random.default_rng(), model_with_ic)\nx = [(; u0 = rand(10),\n         tspan = (tsteps[1], tsteps[end]),\n         saveat = tsteps) for _ in 1:5]\nbatch_preds, _ = model_with_ic(x, ps, st)\nsize(batch_preds)  # (2, 101, 5)","category":"page"},{"location":"","page":"Home","title":"Home","text":"A similar workwflow can be used with ODEModel and AnalyticModel.","category":"page"},{"location":"#Training-with-Optimisers.jl-through-the-SGDBackend","page":"Home","title":"Training with Optimisers.jl through the SGDBackend","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"⚠️ The default train function with the InferICs setup is opiniated and meant for demonstration purposes. You are encouraged to define your own training pipeline.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Lux, Optimisers, ComponentArrays # conditional loading to use `SGDBackend`\nusing Zygote\ndata = rand(2, length(tsteps))\ndataloader = SegmentedTimeSeries((data, tsteps); segment_length = 10, shift = 2)\n\nbackend = SGDBackend(Adam(1e-2), 100, AutoZygote(), MSELoss())\nresult = train(backend, model, dataloader, InferICs(false))\n\n# Make predictions\ntspan = (tsteps[1], tsteps[end])\nprediction, _ = model(\n    (; u0 = result.ics[1].u0,\n        tspan = tspan,\n        saveat = tsteps), result.ps, result.st)","category":"page"},{"location":"#Bayesian-inference-with-Turing.jl-through-the-MCSamplingBackend","page":"Home","title":"Bayesian inference with Turing.jl through the MCSamplingBackend","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using Distributions, Turing, ComponentArrays # conditional loading to use `MCSamplingBackend`\n\n# Add priors to rate parameters\nrate_priors = (\n    growth = arraydist([Normal(0.1, 0.05)]),\n    decay = arraydist([Normal(0.05, 0.02)])\n)\nnn_priors = Normal(0, 1)  # Example prior for NN weights\n\n# Create Bayesian model\nbayesian_model = ARModel(\n    (interaction = BayesianLayer(interaction_layer, nn_priors),\n        rates = BayesianLayer(rate_params, rate_priors)),\n    ar_step;\n    dt = 0.1\n)\n\n# MCMC training\ndatadistrib = Normal\nmcmc_backend = MCSamplingBackend(NUTS(0.65), 500, datadistrib)\nresult = train(mcmc_backend,\n    bayesian_model,\n    dataloader,\n    InferICs(false))\n\n# Sample from posterior\nchains = result.chains\nposterior_samples = sample(bayesian_model, chains, 50)","category":"page"},{"location":"#Documentation","page":"Home","title":"📚 Documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Check out the tutorials and the API in the documentation.","category":"page"},{"location":"#Acknowledgments","page":"Home","title":"🙏 Acknowledgments","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Built on the excellent LuxDL, SciML and TuringLang ecosystems:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Lux.jl for neural networks\nTuring.jl for Bayesian inference\nSciMLSensitivity.jl for automatic differentiation\nOrdinaryDiffEq.jl for differential equations","category":"page"},{"location":"examples/data_loading/#Data-Loading-with-[SegmentedTimeSeries](@ref)","page":"Data Loading with SegmentedTimeSeries","title":"Data Loading with SegmentedTimeSeries","text":"","category":"section"},{"location":"examples/data_loading/","page":"Data Loading with SegmentedTimeSeries","title":"Data Loading with SegmentedTimeSeries","text":"This tutorial demonstrates the SegmentedTimeSeries data loader for batching and segmentation of time series data.","category":"page"},{"location":"examples/data_loading/#Basic-usage","page":"Data Loading with SegmentedTimeSeries","title":"Basic usage","text":"","category":"section"},{"location":"examples/data_loading/","page":"Data Loading with SegmentedTimeSeries","title":"Data Loading with SegmentedTimeSeries","text":"Create synthetic time series data for demonstration:","category":"page"},{"location":"examples/data_loading/","page":"Data Loading with SegmentedTimeSeries","title":"Data Loading with SegmentedTimeSeries","text":"tsteps = 0.0:0.1:20.0\ndata = randn(2, length(tsteps))","category":"page"},{"location":"examples/data_loading/","page":"Data Loading with SegmentedTimeSeries","title":"Data Loading with SegmentedTimeSeries","text":"2×201 Matrix{Float64}:\n -0.0509511   1.95525   0.363907  …  0.156741  -0.62516   -0.288977\n -0.579978   -2.59519  -0.532434     0.453022   0.880291  -1.4362","category":"page"},{"location":"examples/data_loading/","page":"Data Loading with SegmentedTimeSeries","title":"Data Loading with SegmentedTimeSeries","text":"Create a SegmentedTimeSeries dataloader:","category":"page"},{"location":"examples/data_loading/","page":"Data Loading with SegmentedTimeSeries","title":"Data Loading with SegmentedTimeSeries","text":"segment_length = 20\n\ndataloader = SegmentedTimeSeries((data, tsteps);\n                                segment_length)","category":"page"},{"location":"examples/data_loading/","page":"Data Loading with SegmentedTimeSeries","title":"Data Loading with SegmentedTimeSeries","text":"SegmentedTimeSeries\n  Time series length: 201\n  Segment length: 20\n  Shift: 19 (5.0% overlap)\n  Batch size: 1\n  Total segments: 10\n  Total batches: 10","category":"page"},{"location":"examples/data_loading/","page":"Data Loading with SegmentedTimeSeries","title":"Data Loading with SegmentedTimeSeries","text":"Key parameters:","category":"page"},{"location":"examples/data_loading/","page":"Data Loading with SegmentedTimeSeries","title":"Data Loading with SegmentedTimeSeries","text":"segment_length: Number of time points per segment\nshift: Step size between segments (default: segment_length)\nbatchsize: Number of segments per batch","category":"page"},{"location":"examples/data_loading/","page":"Data Loading with SegmentedTimeSeries","title":"Data Loading with SegmentedTimeSeries","text":"The dataloader accepts tuples of arrays where the last dimension represents time:","category":"page"},{"location":"examples/data_loading/","page":"Data Loading with SegmentedTimeSeries","title":"Data Loading with SegmentedTimeSeries","text":"other_data = randn(size(data))\nshift = 10\ndataloader = SegmentedTimeSeries((data, other_data, tsteps);\n                                segment_length,\n                                shift,\n                                batchsize = 2)\n\nfor (i, (segment_data, segment_other_data, segment_times)) in enumerate(dataloader)\n    i > 3 && break\n    println(\"Segment $i: time range $(segment_times[1]) to $(segment_times[end])\")\nend","category":"page"},{"location":"examples/data_loading/","page":"Data Loading with SegmentedTimeSeries","title":"Data Loading with SegmentedTimeSeries","text":"Segment 1: time range 0.0 to 2.9\nSegment 2: time range 2.0 to 4.9\nSegment 3: time range 4.0 to 6.9","category":"page"},{"location":"examples/data_loading/#Segmentation-strategies","page":"Data Loading with SegmentedTimeSeries","title":"Segmentation strategies","text":"","category":"section"},{"location":"examples/data_loading/","page":"Data Loading with SegmentedTimeSeries","title":"Data Loading with SegmentedTimeSeries","text":"The following visualization shows how segment_length and shift parameters affect data segmentation:","category":"page"},{"location":"examples/data_loading/","page":"Data Loading with SegmentedTimeSeries","title":"Data Loading with SegmentedTimeSeries","text":"::warning file=examples/data_loading.jmd,line=9::Assignment to `dataloader`\n in soft scope is ambiguous because a global variable by the same name exis\nts: `dataloader` will be treated as a new local. Disambiguate by using `loc\nal dataloader` to suppress this warning or `global dataloader` to assign to\n the existing global variable.","category":"page"},{"location":"examples/data_loading/","page":"Data Loading with SegmentedTimeSeries","title":"Data Loading with SegmentedTimeSeries","text":"(Image: )","category":"page"},{"location":"examples/data_loading/","page":"Data Loading with SegmentedTimeSeries","title":"Data Loading with SegmentedTimeSeries","text":"Smaller shift values create more overlap between segments, while larger segment_length values capture longer temporal dependencies.","category":"page"},{"location":"examples/data_loading/#Tokenization","page":"Data Loading with SegmentedTimeSeries","title":"Tokenization","text":"","category":"section"},{"location":"examples/data_loading/","page":"Data Loading with SegmentedTimeSeries","title":"Data Loading with SegmentedTimeSeries","text":"Tokenization enables indexed access to individual segments, useful for associating segments with specific initial conditions or metadata:","category":"page"},{"location":"examples/data_loading/","page":"Data Loading with SegmentedTimeSeries","title":"Data Loading with SegmentedTimeSeries","text":"tokenized_dataloader = tokenize(dataloader)\n\n# Get available tokens\ntokens_list = collect(tokens(tokenized_dataloader))\n\n# Access specific segment\ntoken = tokens_list[1]\nsegment_data, segment_times = tokenized_dataloader[token]","category":"page"},{"location":"examples/data_loading/","page":"Data Loading with SegmentedTimeSeries","title":"Data Loading with SegmentedTimeSeries","text":"([-0.050951080448505756 1.9552519426133055 … -0.48608359663994943 1.3826514\n164015518; -0.5799783492249142 -2.5951922331968698 … 0.2642532912210001 1.4\n996516368642125], [-1.390265912740375 -0.961687234288757 … -0.4536702672487\n015 -1.3193446546056486; -0.4906005079581443 0.16134020412607694 … 0.471101\n0490669285 -1.5445325042581899], [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0\n.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9])","category":"page"},{"location":"examples/data_loading/#Shuffling-and-partial-segments","page":"Data Loading with SegmentedTimeSeries","title":"Shuffling and partial segments","text":"","category":"section"},{"location":"examples/data_loading/","page":"Data Loading with SegmentedTimeSeries","title":"Data Loading with SegmentedTimeSeries","text":"By default, segments are processed in deterministic order:","category":"page"},{"location":"examples/data_loading/","page":"Data Loading with SegmentedTimeSeries","title":"Data Loading with SegmentedTimeSeries","text":"for (i, (tok, _)) in enumerate(tokenize(dataloader))\n    i > 5 && break\n    println(\"Batch $i contains tokens: \", tok)\nend","category":"page"},{"location":"examples/data_loading/","page":"Data Loading with SegmentedTimeSeries","title":"Data Loading with SegmentedTimeSeries","text":"Batch 1 contains tokens: [1, 2]\nBatch 2 contains tokens: [3, 4]\nBatch 3 contains tokens: [5, 6]\nBatch 4 contains tokens: [7, 8]\nBatch 5 contains tokens: [9, 10]","category":"page"},{"location":"examples/data_loading/","page":"Data Loading with SegmentedTimeSeries","title":"Data Loading with SegmentedTimeSeries","text":"Enable shuffling for randomized training:","category":"page"},{"location":"examples/data_loading/","page":"Data Loading with SegmentedTimeSeries","title":"Data Loading with SegmentedTimeSeries","text":"rng = Random.MersenneTwister(42)\ndataloader_shuffled = SegmentedTimeSeries((data, tsteps);\n                                         segment_length,\n                                         shift,\n                                         shuffle = true,\n                                         rng = rng)\n\nfor (i, (tok, _)) in enumerate(tokenize(dataloader_shuffled))\n    i > 5 && break\n    println(\"Batch $i contains tokens: \", tok)\nend","category":"page"},{"location":"examples/data_loading/","page":"Data Loading with SegmentedTimeSeries","title":"Data Loading with SegmentedTimeSeries","text":"Batch 1 contains tokens: [13]\nBatch 2 contains tokens: [3]\nBatch 3 contains tokens: [19]\nBatch 4 contains tokens: [16]\nBatch 5 contains tokens: [12]","category":"page"},{"location":"examples/data_loading/","page":"Data Loading with SegmentedTimeSeries","title":"Data Loading with SegmentedTimeSeries","text":"Additional options include partial_segment and partial_batch parameters for handling incomplete segments and batches.","category":"page"},{"location":"examples/mcsampling_example/#Bayesian-inference-with-[MCSamplingBackend](@ref)","page":"Bayesian inference with MCSamplingBackend","title":"Bayesian inference with MCSamplingBackend","text":"","category":"section"},{"location":"examples/mcsampling_example/","page":"Bayesian inference with MCSamplingBackend","title":"Bayesian inference with MCSamplingBackend","text":"This tutorial demonstrates Bayesian inference for a hybrid autoregressive model of the classic hare-lynx predator-prey system, where the predation interaction is learned via neural networks while maintaining mechanistic constraints for birth and death processes. Uncertainty quantification is performed using Markov Chain Monte Carlo sampling.","category":"page"},{"location":"examples/mcsampling_example/","page":"Bayesian inference with MCSamplingBackend","title":"Bayesian inference with MCSamplingBackend","text":"note: Note\nThe train function provided by HybridDynamicModels is an experimental feature, exposed for demonstration purposes. Users are encouraged to implement their own train function to gain more control over the training process; see Overloading the train function. A relevant utility function to build custom training pipelines in a Bayesian inference context is create_turing_model.","category":"page"},{"location":"examples/mcsampling_example/#Importing-necessary-packages","page":"Bayesian inference with MCSamplingBackend","title":"Importing necessary packages","text":"","category":"section"},{"location":"examples/mcsampling_example/","page":"Bayesian inference with MCSamplingBackend","title":"Bayesian inference with MCSamplingBackend","text":"In order to use the MCSamplingBackend, we'll need to manually load Lux, Turing, and ComponentArrays. We additionally load Distributions, Plots, StatsPlots, DataFrames, DelimitedFiles, and HTTP for defining priors, data handling and visualization.","category":"page"},{"location":"examples/mcsampling_example/","page":"Bayesian inference with MCSamplingBackend","title":"Bayesian inference with MCSamplingBackend","text":"using Lux, Distributions, Turing, ComponentArrays\nusing HybridDynamicModels\nusing Random\nusing Plots, StatsPlots\nusing DataFrames, DelimitedFiles, HTTP\n\nconst luxtype = Lux.f64","category":"page"},{"location":"examples/mcsampling_example/","page":"Bayesian inference with MCSamplingBackend","title":"Bayesian inference with MCSamplingBackend","text":"f64 (generic function with 1 method)","category":"page"},{"location":"examples/mcsampling_example/#Data-loading","page":"Bayesian inference with MCSamplingBackend","title":"Data loading","text":"","category":"section"},{"location":"examples/mcsampling_example/","page":"Bayesian inference with MCSamplingBackend","title":"Bayesian inference with MCSamplingBackend","text":"Load the Lynx-Hare population dataset:","category":"page"},{"location":"examples/mcsampling_example/","page":"Bayesian inference with MCSamplingBackend","title":"Bayesian inference with MCSamplingBackend","text":"url = \"http://people.whitman.edu/~hundledr/courses/M250F03/LynxHare.txt\"\ndata = readdlm(IOBuffer(HTTP.get(url).body), ' ') |> luxtype\ndf_data = DataFrame(Year = data[:, 1], Hare = data[:, 2], Lynx = data[:, 3])\n\n# Visualize observed data (hare and lynx)\nplt_data = plot(df_data.Year, df_data.Hare, label = \"Hare\", xlabel = \"Year\",\n    ylabel = \"Population\", title = \"\")\nplot!(plt_data, df_data.Year, df_data.Lynx, label = \"Lynx\")\ndisplay(plt_data)","category":"page"},{"location":"examples/mcsampling_example/","page":"Bayesian inference with MCSamplingBackend","title":"Bayesian inference with MCSamplingBackend","text":"(Image: )","category":"page"},{"location":"examples/mcsampling_example/#Data-preparation","page":"Bayesian inference with MCSamplingBackend","title":"Data preparation","text":"","category":"section"},{"location":"examples/mcsampling_example/","page":"Bayesian inference with MCSamplingBackend","title":"Bayesian inference with MCSamplingBackend","text":"Prepare training and test datasets:","category":"page"},{"location":"examples/mcsampling_example/","page":"Bayesian inference with MCSamplingBackend","title":"Bayesian inference with MCSamplingBackend","text":"tsteps = Vector(df_data.Year)\n\n# Extract hare and lynx data\nhare_lynx_data = Array(df_data[:, Not(:Year)])' |> luxtype\nhare_lynx_data ./= maximum(hare_lynx_data)\n\n# Data array: [hare, lynx]\ndata_array = hare_lynx_data |> luxtype\n\nforecast_length = 10\ntest_idx = size(data_array, 2) - forecast_length + 1:size(data_array, 2)\n\ndata_array_train = data_array[:, Not(test_idx)]\ntsteps_train = tsteps[Not(test_idx)]\n\n# Create training dataloader\ndataloader_train = SegmentedTimeSeries(\n    (data_array_train, tsteps_train);\n    segment_length = 4, shift = 3, partial_segment = true)","category":"page"},{"location":"examples/mcsampling_example/","page":"Bayesian inference with MCSamplingBackend","title":"Bayesian inference with MCSamplingBackend","text":"SegmentedTimeSeries\n  Time series length: 81\n  Segment length: 4\n  Shift: 3 (25.0% overlap)\n  Batch size: 1\n  Total segments: 27\n  Total batches: 27","category":"page"},{"location":"examples/mcsampling_example/#Model-definition","page":"Bayesian inference with MCSamplingBackend","title":"Model definition","text":"","category":"section"},{"location":"examples/mcsampling_example/","page":"Bayesian inference with MCSamplingBackend","title":"Bayesian inference with MCSamplingBackend","text":"Define a hare-lynx predator-prey model where the predation interaction is learned via neural networks, while birth and death processes follow mechanistic rules:","category":"page"},{"location":"examples/mcsampling_example/","page":"Bayesian inference with MCSamplingBackend","title":"Bayesian inference with MCSamplingBackend","text":"# Neural network for hare-lynx predation interactions\nhlsize = 2^2\nneural_interactions = Chain(Dense(2, hlsize, relu),\n                        Dense(hlsize, 1, relu))  # Output: predation rate, enforce non-negativity\n\n# Bayesian ecological parameters with Uniform priors using same bounds as SGD constraints\nmechanistic_priors = (\n    hare_birth = Uniform(0.1, 3.0),\n    hare_death = Uniform(0.1, 3.0),\n    lynx_death = Uniform(1e-3, 2.0)\n)\n\n# Priors for neural network weights\nneural_priors =  Normal(0.0, 1.0)  # Prior for neural network weights\n\n# Hybrid ecosystem dynamics\nfunction ecosystem_step(layers, u, ps, t)\n    hare, lynx = max.(u, 0.)  # Unpack state variables\n    \n    params = layers.mechanistic_params(ps.mechanistic_params)\n    \n    # Neural network: predation rate\n    predation_input = [hare, lynx]\n    predation_rate = layers.neural_interactions(predation_input, ps.neural_interactions)[1]\n    \n    # Mechanistic hare dynamics\n    hare_birth = params.hare_birth[1] * hare\n    hare_predation = -predation_rate * hare * lynx\n    hare_natural_death = -params.hare_death[1] * hare\n    \n    # Mechanistic lynx dynamics\n    lynx_predation_gain = predation_rate * hare * lynx  # Lynx gain from predation\n    lynx_death = -params.lynx_death[1] * lynx\n    \n    # Return derivatives\n    return [\n        hare_birth + hare_predation + hare_natural_death,  # Hare\n        lynx_predation_gain + lynx_death                   # Lynx\n    ]\nend\n\n# Create Bayesian autoregressive model\nmodel = ARModel(\n    (neural_interactions = BayesianLayer(neural_interactions, neural_priors),\n     mechanistic_params = BayesianLayer(ParameterLayer(), mechanistic_priors)),\n    ecosystem_step;\n    dt = tsteps[2] - tsteps[1],\n)","category":"page"},{"location":"examples/mcsampling_example/","page":"Bayesian inference with MCSamplingBackend","title":"Bayesian inference with MCSamplingBackend","text":"ARModel(\n    neural_interactions = BayesianLayer(\n        layers = Chain(\n            layer_1 = Dense(2 => 4, relu),  # 12 parameters\n            layer_2 = Dense(4 => 1, relu),  # 5 parameters\n        ),\n    ),\n    mechanistic_params = BayesianLayer(\n        layers = ParameterLayer{NoConstraint, HybridDynamicModels.var\"#42#4\n4\"{@NamedTuple{}}, HybridDynamicModels.var\"#43#45\"{@NamedTuple{constraint::\n@NamedTuple{}}}}(NoConstraint(), HybridDynamicModels.var\"#42#44\"{@NamedTupl\ne{}}(NamedTuple()), HybridDynamicModels.var\"#43#45\"{@NamedTuple{constraint:\n:@NamedTuple{}}}((constraint = NamedTuple(),))),\n    ),\n)         # Total: 17 parameters,\n          #        plus 0 states.","category":"page"},{"location":"examples/mcsampling_example/#Training-configuration","page":"Bayesian inference with MCSamplingBackend","title":"Training configuration","text":"","category":"section"},{"location":"examples/mcsampling_example/","page":"Bayesian inference with MCSamplingBackend","title":"Bayesian inference with MCSamplingBackend","text":"Configure Bayesian inference with MCMC sampling. We assume a Normal likelihood for the observed data with a fixed standard deviation.","category":"page"},{"location":"examples/mcsampling_example/","page":"Bayesian inference with MCSamplingBackend","title":"Bayesian inference with MCSamplingBackend","text":"# MCMC sampler\nsampler = NUTS(0.65)  # No-U-Turn Sampler\n\n# Likelihood distribution\ndata_distrib = x -> Normal(x, 0.2)\n\nrng = MersenneTwister(42)  # For reproducibility\n\n# Training backend configuration\nbackend = MCSamplingBackend(\n    sampler,      # MCMC sampler\n    1000,          # Number of MCMC samples\n    data_distrib;  # Likelihood distribution\n    rng\n)","category":"page"},{"location":"examples/mcsampling_example/","page":"Bayesian inference with MCSamplingBackend","title":"Bayesian inference with MCSamplingBackend","text":"HybridDynamicModelsTuringExt.MCSamplingBackend(NUTS{AutoForwardDiff{nothing\n, Nothing}, AdvancedHMC.DiagEuclideanMetric}(-1, 0.65, 10, 1000.0, 0.0, Aut\noForwardDiff()), 1000, Main.var\"##WeaveSandBox#513\".var\"#1#2\"(), Base.Pairs\n{Symbol, MersenneTwister, Tuple{Symbol}, @NamedTuple{rng::MersenneTwister}}\n(:rng => MersenneTwister(42)))","category":"page"},{"location":"examples/mcsampling_example/#Training","page":"Bayesian inference with MCSamplingBackend","title":"Training","text":"","category":"section"},{"location":"examples/mcsampling_example/","page":"Bayesian inference with MCSamplingBackend","title":"Bayesian inference with MCSamplingBackend","text":"Train the model with Bayesian inference without inferring initial conditions (to reduce computation time for this example):","category":"page"},{"location":"examples/mcsampling_example/","page":"Bayesian inference with MCSamplingBackend","title":"Bayesian inference with MCSamplingBackend","text":"@info \"Starting Bayesian training...\"\nresult = train(backend, model, dataloader_train, InferICs(false));","category":"page"},{"location":"examples/mcsampling_example/","page":"Bayesian inference with MCSamplingBackend","title":"Bayesian inference with MCSamplingBackend","text":"Starting Bayesian training...\nFound initial step size\n  ϵ = 0.2","category":"page"},{"location":"examples/mcsampling_example/#Results-analysis","page":"Bayesian inference with MCSamplingBackend","title":"Results analysis","text":"","category":"section"},{"location":"examples/mcsampling_example/","page":"Bayesian inference with MCSamplingBackend","title":"Bayesian inference with MCSamplingBackend","text":"Analyze MCMC chains and posterior distributions:","category":"page"},{"location":"examples/mcsampling_example/","page":"Bayesian inference with MCSamplingBackend","title":"Bayesian inference with MCSamplingBackend","text":"# Extract MCMC chains\nchains = result.chains\n\n@info \"MCMC Summary:\"\ndisplay(chains)","category":"page"},{"location":"examples/mcsampling_example/","page":"Bayesian inference with MCSamplingBackend","title":"Bayesian inference with MCSamplingBackend","text":"MCMC Summary:\nChains MCMC chain (1000×34×1 Array{Float64, 3}):\n\nIterations        = 501:1:1500\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 177.01 seconds\nCompute duration  = 177.01 seconds\nparameters        = model_neural_interactions_layer_1_weight[1, 1], model_n\neural_interactions_layer_1_weight[2, 1], model_neural_interactions_layer_1_\nweight[3, 1], model_neural_interactions_layer_1_weight[4, 1], model_neural_\ninteractions_layer_1_weight[1, 2], model_neural_interactions_layer_1_weight\n[2, 2], model_neural_interactions_layer_1_weight[3, 2], model_neural_intera\nctions_layer_1_weight[4, 2], model_neural_interactions_layer_1_bias[1], mod\nel_neural_interactions_layer_1_bias[2], model_neural_interactions_layer_1_b\nias[3], model_neural_interactions_layer_1_bias[4], model_neural_interaction\ns_layer_2_weight[1, 1], model_neural_interactions_layer_2_weight[1, 2], mod\nel_neural_interactions_layer_2_weight[1, 3], model_neural_interactions_laye\nr_2_weight[1, 4], model_neural_interactions_layer_2_bias[1], model_mechanis\ntic_params_hare_birth, model_mechanistic_params_hare_death, model_mechanist\nic_params_lynx_death\ninternals         = n_steps, is_accept, acceptance_rate, log_density, hamil\ntonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree\n_depth, numerical_error, step_size, nom_step_size, lp, logprior, loglikelih\nood\n\nUse `describe(chains)` for summary statistics and quantiles.","category":"page"},{"location":"examples/mcsampling_example/","page":"Bayesian inference with MCSamplingBackend","title":"Bayesian inference with MCSamplingBackend","text":"We plot the posterior distributions of key parameters:","category":"page"},{"location":"examples/mcsampling_example/","page":"Bayesian inference with MCSamplingBackend","title":"Bayesian inference with MCSamplingBackend","text":"# Plot parameter traces\ntrace_plot = plot(chains[[\"model_neural_interactions_layer_1_weight[1, 1]\", \n                            \"model_mechanistic_params_hare_birth\", \n                            \"model_mechanistic_params_hare_death\", \n                            \"model_mechanistic_params_lynx_death\"]], title=\"Parameter Traces\")\nplot!(trace_plot[1, 1], title = \"Neural Network Weight[1,1]\")\nplot!(trace_plot[2, 1], title = \"Hare Birth Rate\")\nplot!(trace_plot[3, 1], title = \"Hare Death Rate\")\nplot!(trace_plot[4, 1], title = \"Lynx Death Rate\")\n[plot!(trace_plot[i, 2], title = \"\") for i in 1:4]\ndisplay(trace_plot)","category":"page"},{"location":"examples/mcsampling_example/","page":"Bayesian inference with MCSamplingBackend","title":"Bayesian inference with MCSamplingBackend","text":"(Image: )","category":"page"},{"location":"examples/mcsampling_example/#Forecasting-with-uncertainty","page":"Bayesian inference with MCSamplingBackend","title":"Forecasting with uncertainty","text":"","category":"section"},{"location":"examples/mcsampling_example/","page":"Bayesian inference with MCSamplingBackend","title":"Bayesian inference with MCSamplingBackend","text":"Forecast on test data with uncertainty quantification. For this, we use ","category":"page"},{"location":"examples/mcsampling_example/","page":"Bayesian inference with MCSamplingBackend","title":"Bayesian inference with MCSamplingBackend","text":"# Function for forecasting with uncertainty\nfunction forecast_with_uncertainty(model, chains, tsteps_test, u0, t0, n_samples)\n    forecasts = []\n    posterior_samples = sample(model, chains, n_samples)\n\n    for i in 1:n_samples\n\n        pred, _ = model((; u0 = u0, tspan = (t0, tsteps_test[end]), saveat = tsteps_test), \n                       posterior_samples[i], result.st_model.st.model)\n        push!(forecasts, pred)\n    end\n    \n    return forecasts\nend\n\ntsteps_test = tsteps[test_idx]\ndata_test = data_array[:, test_idx]\nt0 = result.ics[end].t0\ntsteps_last_segment = tsteps_train[tsteps_train .>= t0]\ndata_last_segment = data_array_train[:, tsteps_train .>= t0]\nu0 = data_array[:, findfirst(==(t0), tsteps)]  # Initial condition at t0\n\ntsteps_pred = union(tsteps_last_segment, tsteps_test)\n\nforecast_predictions = forecast_with_uncertainty(model, chains, tsteps_pred, u0, t0, 100)\n\n# Plot forecast with uncertainty\nforecast_plot = plot(title=\"Bayesian forecast\", xlabel=\"Year\", ylabel=\"Population\", legend=:topright)\n\n# Training data (last part)\nhare_color = \"#ffd166\"\nlynx_color = \"#ef476f\"\n\n# Test data\nscatter!(forecast_plot, tsteps_last_segment, data_last_segment[1, :], \n         label=\"Hare Train\", color=hare_color, markershape=:circle, markersize=4, alpha=0.7)\nscatter!(forecast_plot, tsteps_last_segment, data_last_segment[2, :], \n         label=\"Lynx Train\", color=lynx_color, markershape=:circle, markersize=4, alpha=0.7)\nscatter!(forecast_plot, tsteps_test, data_test[1, :], \n         label=\"Hare Test\", color=hare_color, markershape=:diamond, markersize=6, alpha=0.7)\nscatter!(forecast_plot, tsteps_test, data_test[2, :], \n         label=\"Lynx Test\", color=lynx_color, markershape=:diamond, markersize=6, alpha=0.7)\n\n# Forecast uncertainty\nfor i in 1:length(forecast_predictions)\n    plot!(forecast_plot, tsteps_pred, forecast_predictions[i][1, :], \n          color=hare_color, alpha=0.1, label=i==1 ? \"Hare predicted\" : \"\")\n    plot!(forecast_plot, tsteps_pred, forecast_predictions[i][2, :], \n          color=lynx_color, alpha=0.1, label=i==1 ? \"Lynx predicted\" : \"\")\nend\ndisplay(forecast_plot)","category":"page"},{"location":"examples/mcsampling_example/","page":"Bayesian inference with MCSamplingBackend","title":"Bayesian inference with MCSamplingBackend","text":"(Image: )","category":"page"}]
}
