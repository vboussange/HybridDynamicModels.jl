var documenterSearchIndex = {"docs":
[{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/#Dataloaders","page":"API","title":"Dataloaders","text":"","category":"section"},{"location":"api/#HybridDynamicModels.SegmentedTimeSeries","page":"API","title":"HybridDynamicModels.SegmentedTimeSeries","text":"SegmentedTimeSeries(data; segment_length=2, shift=nothing, batchsize=1, shuffle=false, partial_segment=false, partial_batch=false, rng=GLOBAL_RNG)\n\nAn object that iterates over mini-batches of segments of data, each segment containing segment_length data points, each mini-batch containing batchsize segments. The last dimension in each tensor is the time dimension.\n\nArguments\n\ndata: Input data (array, tuple, or named tuple).\nsegment_length: Number of time points in each segment.\nshift: Step size between consecutive segments (default: segment_length).\nbatchsize: Number of segments per batch.\nshuffle: Whether to shuffle segment order.\npartial_segment: Allow shorter final segments.\npartial_batch: Allow smaller final batches.\nrng: Random number generator for shuffling.\n\nInputs\n\ndata: The time series data to segment.\n\nOutputs\n\nIterator yielding batches of data segments.\n\nBehavior\n\nCreates overlapping or non-overlapping segments from time series data for training dynamical models. Segments can be shuffled and batched for efficient training.\n\nExample\n\njulia> Xtrain = rand(10, 100)\njulia> sdl = SegmentedTimeSeries(Xtrain; segment_length=2, batchsize=1)\njulia> for batch in sdl\n           println(\"Batch: \", summary(batch))\n       end\n\n!!!warning     Undefined behavior when data dimensions are incompatible\n\n\n\n\n\n","category":"type"},{"location":"api/#HybridDynamicModels.tokenize","page":"API","title":"HybridDynamicModels.tokenize","text":"tokenize(sdl::SegmentedTimeSeries)\n\nConvert a SegmentedTimeSeries to use token-based indexing.\n\nArguments\n\nsdl: The SegmentedTimeSeries to tokenize.\n\nInputs\n\nsdl: SegmentedTimeSeries object.\n\nOutputs\n\nTokenized SegmentedTimeSeries with integer-based segment access.\n\nBehavior\n\nTransforms segment indices into a token-based system for easier access to individual segments.\n\nExample\n\njulia> sdl = SegmentedTimeSeries(rand(10, 100); segment_length=2)\njulia> tokenized_sdl = tokenize(sdl)\njulia> tokens(tokenized_sdl) # Returns available tokens\n\n\n\n\n\n","category":"function"},{"location":"api/#HybridDynamicModels.tokens","page":"API","title":"HybridDynamicModels.tokens","text":"tokens(sdl::SegmentedTimeSeries)\n\nGet the available tokens for a tokenized SegmentedTimeSeries.\n\nArguments\n\nsdl: A tokenized SegmentedTimeSeries.\n\nInputs\n\nsdl: Tokenized SegmentedTimeSeries object.\n\nOutputs\n\nRange of available tokens (1 to number of segments).\n\nBehavior\n\nReturns the range of tokens that can be used to access individual segments in a tokenized SegmentedTimeSeries.\n\nExample\n\njulia> sdl = SegmentedTimeSeries(rand(10, 100); segment_length=2)\njulia> tokenized_sdl = tokenize(sdl)\njulia> collect(tokens(tokenized_sdl)) # [1, 2, 3, ...]\n\n\n\n\n\n","category":"function"},{"location":"api/#HybridDynamicModels.create_train_val_loaders","page":"API","title":"HybridDynamicModels.create_train_val_loaders","text":"create_train_val_loaders(data; segment_length, valid_length, kwargs...)\n\nCreate separate training and validation SegmentedTimeSeries loaders from a dataset with the same number of batches.\n\nThis function splits the data into non-overlapping training and validation segments. The training data uses segments with gaps equal to valid_length to leave space for validation segments. The validation data starts after the first training segment and uses segments of length valid_length. Both loaders are guaranteed to have the same number of batches, with tokens referring to the same ordering.\n\nArguments\n\ndata: Input data (can be an array, tuple, or named tuple)\nsegment_length: Size of training segments\nvalid_length: Size of validation segments\nkwargs...: Additional arguments passed to SegmentedTimeSeries constructors\n\nReturns\n\ndataloader_train: SegmentedTimeSeries for training data\ndataloader_valid: SegmentedTimeSeries for validation data\n\nExamples\n\nWith array data\n\ndata = rand(10, 100)  # 10 features, 100 time steps\ntrain_loader, val_loader = create_train_val_loaders(data; \n    segment_length=20, valid_length=5, batchsize=4)\n# Both loaders will have the same number of batches\n@assert length(train_loader) == length(val_loader)\n\nWith tuple data (data, time steps)\n\ndata = rand(10, 100)\ntsteps = 1:100\ntrain_loader, val_loader = create_train_val_loaders((data, tsteps); \n    segment_length=20, valid_length=5, batchsize=4)\n\nWith named tuple data\n\ndataset = (observations = rand(10, 100), times = 1:100, metadata = rand(5, 100))\ntrain_loader, val_loader = create_train_val_loaders(dataset; \n    segment_length=20, valid_length=5)\n\nNotes\n\nTraining segments are spaced segment_length + valid_length apart to avoid overlap with validation\nValidation segments start at position segment_length + 1 to avoid overlap with first training segment\nBoth loaders have partial_segment = false and partial_batch = false to ensure consistent sizes\nBoth loaders are guaranteed to have the same number of batches for synchronized training/validation\n\n\n\n\n\n","category":"function"},{"location":"api/#Parameter-Layers","page":"API","title":"Parameter Layers","text":"","category":"section"},{"location":"api/#HybridDynamicModels.ParameterLayer","page":"API","title":"HybridDynamicModels.ParameterLayer","text":"ParameterLayer(;constraint::AbstractConstraint = NoConstraint(), \n                init_value = (;), \n                init_state_value = (;))\n\nA layer representing parameters, optionally with constraints.\n\nArguments\n\nconstraint: Constraint to transform parameters.\ninit_value: Initial parameter values (NamedTuple or AbstractArray).\ninit_state_value: Internal state (NamedTuple).\n\nInputs\n\nps: Parameters of the layer.\nst: States of the layer.\n\nOutputs\n\nConstrained parameter values merged with states.\n\nBehavior\n\nApplies constraints to parameters during forward pass. Parameters are transformed from unconstrained to constrained space.\n\nExample\n\njulia> param = ParameterLayer(; constraint = NoConstraint(),\n                            init_value = (;a = ones(2)), \n                            init_state_value = (;b = (0.0, 1.0)))\njulia> ps, st = Lux.setup(Random.default_rng(), param)\njulia> x, _ = param(ps, st)\njulia> x == (a = [1.0, 1.0], b = (0.0, 1.0))\ntrue\n\n\n\n\n\n","category":"type"},{"location":"api/#HybridDynamicModels.NoConstraint","page":"API","title":"HybridDynamicModels.NoConstraint","text":"NoConstraint()\n\nApplies no transformation to parameters.\n\nArguments\n\nNone.\n\nInputs\n\nx: Parameter values.\nst: States.\n\nOutputs\n\nUnmodified parameter values and states.\n\nBehavior\n\nIdentity transformation - parameters remain unconstrained.\n\nExample\n\njulia> constraint = NoConstraint()\njulia> x, st = constraint([1.0, 2.0], (;))\n([1.0, 2.0], (;))\n\n\n\n\n\n","category":"type"},{"location":"api/#HybridDynamicModels.BoxConstraint","page":"API","title":"HybridDynamicModels.BoxConstraint","text":"BoxConstraint(lb::AbstractArray, ub::AbstractArray)\n\nConstrains parameters to lie within specified bounds using sigmoid transformation.\n\nArguments\n\nlb: Lower bounds array.\nub: Upper bounds array.\n\nInputs\n\ny: Unconstrained parameter values.\nst: States containing bounds.\n\nOutputs\n\nConstrained parameter values within bounds.\n\nBehavior\n\nTransforms unconstrained parameters to constrained space using sigmoid function.\n\nExample\n\njulia> constraint = BoxConstraint([0.0], [1.0])\njulia> x, st = constraint([0.0], (;lb=[0.0], ub=[1.0]))\n([0.5], (;lb=[0.0], ub=[1.0]))\n\n\n\n\n\n","category":"type"},{"location":"api/#HybridDynamicModels.NamedTupleConstraint","page":"API","title":"HybridDynamicModels.NamedTupleConstraint","text":"NamedTupleConstraint(constraints::NamedTuple)\n\nApplies different constraints to different fields of a NamedTuple.\n\nArguments\n\nconstraints: NamedTuple of constraint objects.\n\nInputs\n\nx: NamedTuple of parameter values.\nst: NamedTuple of states.\n\nOutputs\n\nConstrained parameter values.\n\nBehavior\n\nApplies field-specific constraints to NamedTuple parameters.\n\nExample\n\njulia> constraints = (a = BoxConstraint([0.0], [1.0]), b = NoConstraint())\njulia> constraint = NamedTupleConstraint(constraints)\n\n\n\n\n\n","category":"type"},{"location":"api/#Models","page":"API","title":"Models","text":"","category":"section"},{"location":"api/#HybridDynamicModels.ODEModel","page":"API","title":"HybridDynamicModels.ODEModel","text":"ODEModel(layers::NamedTuple, dudt::Function; kwargs...)\n\nWraps an ODE model for simulation using Lux layers.\n\nArguments\n\nlayers: NamedTuple of Lux layers representing the layers of the model.\ndudt: Function that computes the derivative of the state, with signature dudt(layers, u, ps, t).\nkwargs: Additional keyword arguments passed to the solver (e.g., tspan, saveat, alg).\n\nInputs\n\n(x, ps, st)\nx: a NamedTuple or AbstractVector{NamedTuple} (batch mode).\nps: Parameters of the model.\nst: States of the model.\nA tuple of (x, ps, st): batch mode.\n(ps, st): If x not provided, defaults to kwargs.\n\nOutputs\n\n(sol, st)\nsol: Solution of the ODE problem, with second dimension corresponding to time and batches stacked along the third dimension, if applicable.\nst: Updated states of the model.\n\nBehavior\n\nlayers are wrapped in StatefulLuxLayers to maintain their states. The derivative function dudt should be defined as dudt(layers, u, ps, t) where u is the current state and t is the current time. The function returns the derivative of the state.\n\nExample\n\njulia> layers = (; layer1 = Lux.Dense(10, 10, relu))\njulia> dudt(layers, u, ps, t) = layers.layer1(u, ps.layer1)[1]\njulia> ode_model = ODEModel(layers, dudt, tspan = (0f0, 1f0), saveat = range(0f0, stop=1f0, length=100), alg = Tsit5())\njulia> ps, st = Lux.setup(Random.default_rng(), ode_model)\njulia> ode_model((; u0 = ones(Float32, 10)), ps, st)\n\n!!!warning     Undefined behavior when ps is not a NamedTuple\n\n\n\n\n\n","category":"type"},{"location":"api/#HybridDynamicModels.AnalyticModel","page":"API","title":"HybridDynamicModels.AnalyticModel","text":"AnalyticModel(layers::NamedTuple, fun::Function; kwargs...)\n\nWraps an analytic model for direct evaluation using Lux layers.\n\nArguments\n\nlayers: NamedTuple of Lux layers representing the layers of the model.\nfun: Function that computes the analytic solution, with signature fun(layers, u0, t0, ps, t).\nkwargs: Additional keyword arguments (e.g., default values for u0, tspan, saveat).\n\nInputs\n\n(x, ps, st)\nx: a NamedTuple or AbstractVector{NamedTuple} (batch mode).\nps: Parameters of the model.\nst: States of the model.\nA tuple of (x, ps, st): batch mode.\n(ps, st): If x not provided, defaults to kwargs.\n\nOutputs\n\n(sol, st)\nsol: Solution array evaluated at specified time points, with second dimension corresponding to time and batches stacked along the third dimension, if applicable.\nst: Updated states of the model.\n\nBehavior\n\nlayers are wrapped in StatefulLuxLayer to maintain their states. The analytic function fun should be defined as fun(layers, u0, t0, ps, t) where t can be a vector of time points and t0 is extracted from tspan.\n\nExample\n\njulia> layers = (; params = ParameterLayer(init_value = (a = 1.0, b = 0.5)))\njulia> analytic_solution(layers, u0, t0, ps, t) = u0 .* exp.(layers.params(ps.params)[1].a .* (t .- t0))\njulia> model = AnalyticModel(layers, analytic_solution; u0 = [1.0], tspan = (0.0, 1.0), saveat = 0:0.1:1.0)\njulia> ps, st = Lux.setup(Random.default_rng(), model)\njulia> model((; u0 = [1.0]), ps, st)\n\n!!!warning     Undefined behavior when ps is not a NamedTuple\n\n\n\n\n\n","category":"type"},{"location":"api/#HybridDynamicModels.ARModel","page":"API","title":"HybridDynamicModels.ARModel","text":"ARModel(layers::NamedTuple, fun::Function; kwargs...)\n\nWraps an autoregressive (AR) model.\n\nArguments\n\nlayers: NamedTuple of Lux layers representing the layers associated with the model.\nfun: Function that computes the next time step, with signature fun(layers, u, ps, t).\nkwargs: Additional keyword arguments (e.g., default values for u0, tspan, saveat, dt).\n\nInputs\n\n(x, ps, st)\nx: a NamedTuple or AbstractVector{NamedTuple} (batch mode).\nps: Parameters of the model.\nst: States of the model.\nA tuple of (x, ps, st): batch mode.\n(ps, st): If x not provided, defaults to kwargs.\n\nOutputs\n\n(sol, st)\nsol: Solution array with iterative predictions, with second dimension corresponding to time and batches stacked along the third dimension, if applicable.\nst: Updated states of the model.\n\nBehavior\n\nlayers are wrapped in StatefulLuxLayer to maintain their states. The AR function fun should be defined as fun(layers, u, ps, t) where:\n\nu is the current state\nt is the current time\nThe function returns the next state\n\nThe model iteratively applies the function to generate a time series from initial conditions.\n\nExample\n\njulia> using HybridDynamicModels, Lux, Random\n\njulia> layers = (; \n           predictor = Dense(2, 2), \n           params = ParameterLayer(init_value = (; decay = [95f-2],))\n       );\n\njulia> ar_step(layers, u, ps, t) = layers.predictor(u, ps.predictor) .* layers.params(ps.params).decay;\n\njulia> model = ARModel(layers, ar_step; dt = 1f-1, u0 = [1f0, 5f-1], tspan = (0f0, 1f0), saveat = 0:1f-1:1f0);\n\njulia> ps, st = Lux.setup(Random.default_rng(), model);\n\njulia> x = (; u0 = [1f0, 5f-1]);\n\njulia> y, st = model(x, ps, st);\n\njulia> size(y) # 2 state variables, 11 time points\n(2, 11) \n\n!!!warning     Undefined behavior when x is not a NamedTuple\n\n\n\n\n\n","category":"type"},{"location":"api/#Initial-Conditions","page":"API","title":"Initial Conditions","text":"","category":"section"},{"location":"api/#HybridDynamicModels.ICLayer","page":"API","title":"HybridDynamicModels.ICLayer","text":"ICLayer(ics::AbstractLuxLayer)\nICLayer(ics::<:ParameterLayer)\nICLayer(ics::Vector{<:ParameterLayer})\n\nInitial condition layer.\n\nArguments\n\nics: Lux layer, ParameterLayer, or vector of ParameterLayer.\n\nInputs\n\n(x, ps, st) with x a NamedTuple or AbstractVector{NamedTuple} (batch mode).\n(ps, st) when ics is a ParameterLayer.\n\nOutputs\n\nInitial conditions merged with other fields.\nUpdated states.\n\nBehavior\n\nProcesses initial conditions through wrapped layers and merges with input data.\n\nExample\n\njulia> ic_layer = ICLayer(ParameterLayer(init_value = (;u0 = [1.0])))\n\n\n\n\n\n","category":"type"},{"location":"api/#Training-API","page":"API","title":"Training API","text":"","category":"section"},{"location":"api/#HybridDynamicModels.train","page":"API","title":"HybridDynamicModels.train","text":"train(backend::AbstractOptimBackend, model, dataloader::SegmentedTimeSeries, infer_ics::InferICs, rng=Random.default_rng(); pstype=Lux.f64)\n\nTrain a dynamical model using segmented time series data.\n\nArguments\n\nbackend: Training configuration and optimization settings.\nmodel: Lux model to train.\ndataloader: Time series data split into segments.\ninfer_ics: Initial condition inference configuration.\nrng: Random number generator.\npstype: Precision type for parameters.\n\nInputs\n\nbackend: Training backend (SGDBackend, MCSamplingBackend, etc.).\nmodel: Model to train.\ndataloader: SegmentedTimeSeries data.\ninfer_ics: InferICs configuration.\n\nOutputs\n\nNamedTuple with training results (varies by backend).\n\nBehavior\n\nTrains model using specified backend on segmented time series data.\n\nExample\n\njulia> backend = SGDBackend(Adam(1e-3), 1000, AutoZygote(), MSELoss())\njulia> dataloader = SegmentedTimeSeries(data, segment_length=20)\njulia> infer_ics = InferICs(true)\njulia> result = train(backend, model, dataloader, infer_ics)\n\n\n\n\n\n","category":"function"},{"location":"api/#HybridDynamicModels.InferICs","page":"API","title":"HybridDynamicModels.InferICs","text":"InferICs(infer::Bool, u0_constraint=NoConstraint())\n\nConfiguration for initial condition inference in training.\n\nArguments\n\ninfer: Whether to treat initial conditions as learnable parameters.\nu0_constraint: Constraint for initial condition optimization.\n\nInputs\n\ninfer: Boolean flag for inference.\nu0_constraint: Constraint object.\n\nOutputs\n\nInferICs configuration object.\n\nBehavior\n\nControls whether initial conditions are learned or fixed during training.\n\nExample\n\njulia> infer_ics = InferICs(true, NoConstraint())\n\n\n\n\n\n","category":"type"},{"location":"api/#Lux.jl-training-backend","page":"API","title":"Lux.jl training backend","text":"","category":"section"},{"location":"api/#HybridDynamicModels.SGDBackend","page":"API","title":"HybridDynamicModels.SGDBackend","text":"SGDBackend(opt, n_epochs, adtype, loss_fn; verbose_frequency=10, callback=(l,m,p,s)->nothing)\n\nTraining backend using Lux.jl for mode estimation.\n\nArguments\n\nopt: Optimizers.jl rule for parameter updates.\nn_epochs: Number of training epochs.\nadtype: Automatic differentiation backend.\nloss_fn: Loss function for training.\ncallback: User-defined callback function.\n\nInputs\n\nopt: Optimization rule (e.g., Adam(1e-3)).\nn_epochs: Total training epochs.\nadtype: AD backend (e.g., AutoZygote()).\nloss_fn: Loss function.\ncallback: Optional callback.\n\nOutputs\n\nNamedTuple with trained parameters and states.\n\nBehavior\n\nUses stochastic gradient descent for maximum likelihood estimation.\n\nExample\n\njulia> backend = SGDBackend(Adam(1e-3), 1000, AutoZygote(), MSELoss())\n\n\n\n\n\n","category":"type"},{"location":"api/#Turing.jl-backend","page":"API","title":"Turing.jl backend","text":"","category":"section"},{"location":"api/#HybridDynamicModels.BayesianLayer","page":"API","title":"HybridDynamicModels.BayesianLayer","text":"BayesianLayer(layer, priors)\n\nWrapper layer that adds Bayesian priors to Lux layers for probabilistic modeling.\n\nArguments\n\nlayer: Lux layer to make Bayesian.\npriors: Prior distributions for parameters.\n\nInputs\n\nlayer: Any Lux layer.\npriors: Distribution or NamedTuple of distributions.\n\nOutputs\n\nLayer with Bayesian priors for MCMC inference.\n\nBehavior\n\nEnables probabilistic modeling by attaching priors to layer parameters.\n\nExample\n\njulia> dense_layer = Dense(10, 5)\njulia> bayesian_dense = BayesianLayer(dense_layer, Normal(0, 1))\n\n\n\n\n\n","category":"type"},{"location":"api/#HybridDynamicModels.getpriors","page":"API","title":"HybridDynamicModels.getpriors","text":"getpriors(layer)\n\nExtract prior distributions from Bayesian layers.\n\nArguments\n\nlayer: Layer or model containing BayesianLayer components.\n\nInputs\n\nlayer: Bayesian or composite layer.\n\nOutputs\n\nNamedTuple of prior distributions.\n\nBehavior\n\nRecursively extracts priors from Bayesian layers in model hierarchy.\n\nExample\n\njulia> priors = getpriors(bayesian_model)\n\n\n\n\n\n","category":"function"},{"location":"api/#HybridDynamicModels.MCSamplingBackend","page":"API","title":"HybridDynamicModels.MCSamplingBackend","text":"MCSamplingBackend(sampler, n_iterations, datadistrib; kwargs...)\n\nTraining backend for Bayesian inference using Monte Carlo sampling.\n\nArguments\n\nsampler: Turing.jl MCMC sampler.\nn_iterations: Number of MCMC samples.\ndatadistrib: Data distribution for likelihood.\nkwargs: Additional sampler options.\n\nInputs\n\nsampler: MCMC sampling algorithm.\nn_iterations: Total posterior samples.\ndatadistrib: Distribution for data likelihood.\n\nOutputs\n\nNamedTuple with MCMC chains and model state.\n\nBehavior\n\nPerforms Bayesian inference using MCMC sampling on models with priors.\n\nExample\n\njulia> backend = MCSamplingBackend(NUTS(0.65), 1000, LogNormal)\n\n\n\n\n\n","category":"type"},{"location":"dev_guide/","page":"-","title":"-","text":"<!– To be completed –>","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: Build Status)","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: )","category":"page"},{"location":"#HybridDynamicModels.jl","page":"Home","title":"HybridDynamicModels.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A comprehensive toolbox for hybrid dynamical modeling combining domain knowledge with machine learning.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"HybridDynamicModels.jl provides a unified framework for building, training, and analyzing hybrid dynamical models that seamlessly integrate traditional scientific models with machine learning layers. Built on Lux.jl, it enables both gradient-based optimization and Bayesian inference for uncertainty quantification.","category":"page"},{"location":"#Key-Features","page":"Home","title":"🚀 Key Features","text":"","category":"section"},{"location":"#**Dynamical-model-layers**","page":"Home","title":"Dynamical model layers","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ICLayer: For initial condition inference\nODEModel: Neural ODEs\nARModel: Autoregressive models\nAnalyticModel: For explicit dynamical models","category":"page"},{"location":"#**Utility-layers-for-hybrid-modeling**","page":"Home","title":"Utility layers for hybrid modeling","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ParameterLayer: Learnable parameters, composable with optional Constraint layers\nBayesianLayer: Add probabilistic priors to any Lux layer","category":"page"},{"location":"#**Data-loaders**","page":"Home","title":"Data loaders","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"SegmentedTimeSeries: Time series data loader with segmentation, implementing mini-batching.","category":"page"},{"location":"#**Training-API,-with-following-backends**","page":"Home","title":"Training API, with following backends","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"SGDBackend: Fast gradient-based optimization with automatic differentiation\nMCSamplingBackend: Full Bayesian inference with uncertainty quantification  \nVIBackend: Variational inference for scalable approximate Bayesian methods","category":"page"},{"location":"#Installation","page":"Home","title":"📦 Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"HybridDynamicModels\")","category":"page"},{"location":"#Quick-Start","page":"Home","title":"🔥 Quick Start","text":"","category":"section"},{"location":"#Basic-Hybrid-ODE-Model","page":"Home","title":"Basic Hybrid ODE Model","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using HybridDynamicModels\nusing Lux, OrdinaryDiffEq, Optimisers\n\n# Define hybrid model layers\nneural_layer = Chain(Dense(2, 2, tanh), Dense(2, 2))\n\nparam_constraint = BoxConstraint([1e-2], [1e0])\nparam_layer = ParameterLayer(init_value = (growth_rate = [0.1, 0.2]), constraint = param_constraint)\n\n# Hybrid dynamics combining neural network and domain knowledge\nfunction dudt(layers, u, ps, t)\n    # Domain-specific term\n    domain_params = layers.params(ps.params)\n    growth_term = domain_params.growth_rate .* u\n    \n    # Neural network term  \n    neural_term = layers.neural(u, ps.neural)\n    \n    return growth_term + neural_term\nend\n\n# Create the ODE model\nmodel = ODEModel(\n    (neural = neural_layer, params = param_layer),\n    dudt,\n    alg = Tsit5(),\n    abstol = 1e-6,\n    reltol = 1e-6\n)\n\n# Setup training data\ndata = rand(2, 100)  # Your time series data\ndataloader = SegmentedTimeSeries(data; segment_length=20, shift=10)\n\n# Configure training\nbackend = SGDBackend(\n    Adam(1e-3),           # Optimizer\n    1000,                 # Number of epochs  \n    AutoZygote(),         # AD backend\n    MSELoss()            # Loss function\n)\n\n# Train the model\nresult = train(backend, model, dataloader, InferICs(false))\ntrained_model = result.best_model\n\n# Make predictions\nprediction = trained_model((u0 = [1.0, 0.5], tspan = (0.0, 10.0), saveat = 0:0.1:10))","category":"page"},{"location":"#Bayesian-Parameter-Estimation","page":"Home","title":"Bayesian Parameter Estimation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using Distributions, Turing\n\n# Add Bayesian priors to parameters\nparam_priors = (growth_rate = product_distribution([Normal(0.1, 0.05), Normal(0.2, 0.05)]),)\nbayesian_params = BayesianLayer(param_layer, param_priors)\n\n# Create Bayesian model\nbayesian_model = ODEModel(\n    (neural = neural_layer, params = bayesian_params),\n    dudt,\n    alg = Tsit5()\n)\n\n# MCMC training for uncertainty quantification\nmcmc_backend = MCSamplingBackend(\n    NUTS(0.65),          # MCMC sampler\n    1000,                # Number of samples\n    LogNormal            # Data likelihood\n)\n\n# Bayesian training\nresult = train(mcmc_backend, bayesian_model, dataloader, InferICs(true))\nchains = result.chains\n\n# Posterior analysis\nposterior_samples = sample(result.st_model, chains, 100)","category":"page"},{"location":"#Learning-Initial-Conditions","page":"Home","title":"Learning Initial Conditions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"# Configure learnable initial conditions with constraints\nconstraint_u0 = BoxConstraint([1e-3], [5e0])  # Positive initial conditions\ninfer_ics = InferICs(true, constraint_u0)\n\n# Train with learned initial conditions\nresult = train(backend, model, dataloader, infer_ics)\n\n# Access learned initial conditions for each segment\nfor (i, token) in enumerate(tokens(tokenize(dataloader)))\n    ic_params = result.best_model.initial_conditions.ics[i]\n    println(\"Segment $i initial condition: \", ic_params)\nend","category":"page"},{"location":"#Documentation","page":"Home","title":"📚 Documentation","text":"","category":"section"},{"location":"#Examples","page":"Home","title":"Examples","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"TO COMPLETE","category":"page"},{"location":"#API","page":"Home","title":"API","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Checkout the API documentation.","category":"page"},{"location":"#Acknowledgments","page":"Home","title":"🙏 Acknowledgments","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Built on the excellent LuxDL, SciML and TuringLang ecosystem, particularly:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Lux.jl for neural networks\nTuring.jl for Bayesian inference\nSciMLSensitivity.jl for automatic differentiation\nOrdinaryDiffEq.jl for differential equations","category":"page"}]
}
