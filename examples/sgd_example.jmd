# Training with [`SGDBackend`](@ref)

This tutorial demonstrates how to use the [`SGDBackend`](@ref) for training a hybrid autoregressive model modelling the classic hare-lynx predator-prey system, where the predation interaction is learned via neural networks while maintaining mechanistic constraints for birth and death processes.

!!! note
    The [`train`](@ref) function provided by `HybridDynamicModels` is an experimental feature, exposed for demonstration purposes. Users are encouraged to implement their own `train` function to gain more control over the training process; see [Overloading the `train` function](@ref).

## Importing necessary packages
In order to use the SGDBackend, we'll need to manually load `Lux`, `Optimisers`, and `ComponentArrays`. We additionally load `Zygote` for automatic differentiation, `ParameterSchedulers` for learning rate scheduling, and `Plots`, `DataFrames`, `DelimitedFiles`, and `HTTP` for data handling and visualization.

```julia
using Lux, Optimisers, ComponentArrays
using Zygote
using HybridDynamicModels
using ParameterSchedulers
using Random
using Plots
using DataFrames, DelimitedFiles, HTTP

const luxtype = Lux.f64
```

## Data Loading

Load the Lynx-Hare population dataset:

```julia
url = "http://people.whitman.edu/~hundledr/courses/M250F03/LynxHare.txt"
data = readdlm(IOBuffer(HTTP.get(url).body), ' ') |> luxtype
df_data = DataFrame(Year = data[:, 1], Hare = data[:, 2], Lynx = data[:, 3])

# Visualize observed data (hare and lynx)
plt_data = plot(df_data.Year, df_data.Hare, label = "Hare", xlabel = "Year",
    ylabel = "Population", title = "Observed Hare-Lynx Data")
plot!(plt_data, df_data.Year, df_data.Lynx, label = "Lynx")
display(plt_data)
```

## Data Preparation

Prepare training and test datasets:

```julia
tsteps = Vector(df_data.Year) |> luxtype

# Extract hare and lynx data
hare_lynx_data = Array(df_data[:, Not(:Year)])' |> luxtype
hare_lynx_data ./= maximum(hare_lynx_data)

# Data array: [hare, lynx]
data_array = hare_lynx_data |> luxtype

forecast_length = 20
test_idx = size(data_array, 2) - forecast_length + 1:size(data_array, 2)

# Create training dataloader
dataloader_train = SegmentedTimeSeries(
    (data_array[:, Not(test_idx)], tsteps[Not(test_idx)]);
    segment_length = 4, shift = 2, batchsize = 20)
```

## Model Definition

Define a hare-lynx predator-prey model where the predation interaction is learned via neural networks, while birth and death processes follow mechanistic rules:

```julia
# Neural network for hare-lynx predation interactions
hlsize = 2^4
neural_interactions = Chain(Dense(2, hlsize, relu),
                        Dense(hlsize, hlsize, relu),
                        Dense(hlsize, 1))  # Output: predation rate

# Learnable ecological parameters
mechanistic_params = ParameterLayer(init_value = (
                                    hare_birth = [0.8],
                                    hare_death = [0.1],
                                    lynx_death = [0.2] ), 
                                    constraint = NamedTupleConstraint((hare_birth = BoxConstraint([0.0], [2.0]),
                                                                       hare_death = BoxConstraint([0.001], [1.0]),
                                                                       lynx_death = BoxConstraint([0.001], [1.0]))
                                ))

# Hybrid ecosystem dynamics
function ecosystem_step(layers, u, ps, t)
    hare, lynx = max.(u, 0.)  # Unpack state variables
    
    params = layers.mechanistic_params(ps.mechanistic_params)
    
    # Neural network: predation rate
    predation_input = [hare, lynx]
    predation_rate = layers.neural_interactions(predation_input, ps.neural_interactions)[1]
    
    # Mechanistic hare dynamics
    hare_birth = params.hare_birth[1] * hare
    hare_predation = -predation_rate * hare * lynx
    hare_natural_death = -params.hare_death[1] * hare
    
    # Mechanistic lynx dynamics
    lynx_predation_gain = predation_rate * hare * lynx  # Lynx gain from predation
    lynx_death = -params.lynx_death[1] * lynx
    
    # Return derivatives
    return [
        hare_birth + hare_predation + hare_natural_death,  # Hare
        lynx_predation_gain + lynx_death                   # Lynx
    ]
end

# Create autoregressive model
model = ARModel(
    (;neural_interactions, mechanistic_params),
    ecosystem_step;
    dt = tsteps[2] - tsteps[1],
)
```

## Training configuration

Configure training with learning rate scheduling and callbacks:

```julia
# Learning rate schedule: exponential decay
lr_schedule = Step(1e-2, 0.9, 200)

# Callback for monitoring and learning rate adjustment
function callback(loss, epoch, ts)
    if epoch % 20 == 0
        current_lr = lr_schedule(epoch)
        @info "Epoch $epoch: Loss = $loss, LR = $current_lr"
        Optimisers.adjust!(ts.optimizer_state, current_lr)
    end
end

# Training backend configuration
backend = SGDBackend(
    AdamW(eta = 1e-2, lambda = 1e-4),  # Optimizer with weight decay
    2000,                             # Number of epochs
    AutoZygote(),                     # Automatic differentiation
    MSELoss(),                        # Loss function
    callback                          # Training callback
)
```

## Training

Train the model with initial condition inference:

```julia
@info "Starting training..."
result = train(backend, model, dataloader_train, InferICs(true))
```

## Results Visualization

Visualize training fit and test predictions for the hare-lynx ecosystem:

```julia

# Colors: blue for hare, red for lynx
hare_color = "#ffd166"
lynx_color = "#ef476f"

# Function to plot training results
function plot_training_results(dataloader, result, model)
    plt = plot(title = "Training Results", xlabel = "Year",
        ylabel = "Population", legend = :topright)

    dataloader_tokenized = tokenize(dataloader)

    for tok in tokens(dataloader_tokenized)
        segment_data, segment_tsteps = dataloader_tokenized[tok]
        ics = result.ics[tok].u0

        pred, _ = model(
            (; u0 = ics, saveat = segment_tsteps,
                tspan = (segment_tsteps[1], segment_tsteps[end])),
            result.ps, result.st)

        # Plot observed data
        scatter!(plt, segment_tsteps, segment_data[1, :],
            label = (tok == 1 ? "Hare Data" : ""),
            color = hare_color, markersize = 4, alpha = 0.7)
        scatter!(plt, segment_tsteps, segment_data[2, :],
            label = (tok == 1 ? "Lynx Data" : ""),
            color = lynx_color, markersize = 4, alpha = 0.7)

        # Plot predictions
        plot!(plt, segment_tsteps, pred[1, :],
            label = (tok == 1 ? "Hare Predicted" : ""),
            color = hare_color, linewidth = 2)
        plot!(plt, segment_tsteps, pred[2, :],
            label = (tok == 1 ? "Lynx Predicted" : ""),
            color = lynx_color, linewidth = 2)
    end
    return plt
end

# Plot training results
plt_train = plot_training_results(dataloader_train, result, model)
```

Forecast on test data:

```julia
tsteps_test = tsteps[test_idx]
data_test = data_array[:, test_idx]
u0, t0 = result.ics[end]

preds, _ = model((; u0 = u0, tspan = (t0, tsteps_test[end]), saveat = tsteps_test),
                result.ps, result.st)

# Plot test predictions
plt_test = plot(title = "Test Predictions", xlabel = "Year", ylabel = "Population", legend = :topright)
scatter!(plt_test, tsteps_test, data_test[1, :], label = "Hare Data", color = hare_color, markersize = 4, alpha = 0.7)
scatter!(plt_test, tsteps_test, data_test[2, :], label = "Lynx Data", color = lynx_color, markersize = 4, alpha = 0.7)
plot!(plt_test, tsteps_test, preds[1, :], label = "Hare Predicted", color = hare_color, linewidth = 2)
plot!(plt_test, tsteps_test, preds[2, :], label = "Lynx Predicted", color = lynx_color, linewidth = 2)
```

## Some final notes

- When training a neural network-based parametrization, it is usually best practice to use a validation loss to avoid overfitting. This can be implemented by creating a separate validation dataloader (see [`create_train_val_loaders`](@ref)) and modifying the training loop to compute validation loss at intervals, overloading the `train` function. Check out the [Overloading the `train` function](@ref) for an example.